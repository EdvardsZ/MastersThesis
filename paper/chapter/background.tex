\chapter{Background}

In this chapter I am going to introduce the concepts that are necessary to understand the research presented in this thesis. The chapter is divided into four sections. The first section provides an overview of Variational Autoencoders (VAEs) and their applications. The second section introduces Vector Quantized VAEs (VQ-VAEs) and PixelCNN. The third section introduces the concept of semi-conditioned VAEs. The chapter is concluded with a section that introduces the concept of multitask learning. 

\section{VAEs}

Variational Autoencoders (VAEs), first introduced in 2013 by Kingma and Welling~\cite{kingma2013autoencoding}, have become a prominent class of generative models in the field of machine learning.  At their core, VAEs consist of an encoder network with parameters $\phi$ that maps data points $x$ into a latent space $z$ and a decoder network with parameters $\theta$ that generates data $\hat{x}$ from latent representations~\cite{Kingma_2019}. 

The key innovation that makes VAEs work is the introduction of a probabilistic interpretation of the latent space. More specifically, VAEs assume that the latent space $z$ is a random variable that follows a certain prior distribution $p(z)$, which is typically a Gaussian distribution and that the mapping from the latent space to the data space is also probabilistic~\cite{kingma2013autoencoding}.

The optimization target for VAEs is the evidence lower bound (ELBO), which is
 \[ L_{\theta, \phi}(x) = \mathbb{E}_{q_{\phi}(z|x)} [\log p_{\theta}(x, z) - \log q_{\phi}(z|x)], \]
where $q_{\phi}(z|x)$ is the encoder distribution, $p_{\theta}(x, z)$ is the decoder distribution. 

The ELBO can be also written as a sum of two terms,
 \[ L_{\theta, \phi}(x) = - D_{KL}(q_{\phi}(z|x) || p(z)) + \mathbb{E}_{q_{\phi}(z|x)} [\log p_{\theta}(x|z)], \]
 where:

\begin{itemize}
    \item $D_{KL}(q_{\phi}(z|x) || p(z))$ is the Kullback-Leibler divergence between the encoder distribution $q_{\phi}(z|x)$ and the prior distribution $p(z)$
    \item $\mathbb{E}_{q_{\phi}(z|x)} [\log p_{\theta}(x|z)]$ is the reconstruction term
\end{itemize}

The Kullback-Leibler divergence term encourages the encoder (posterior) distribution to align with the prior distribution, acting as a regularization term. This alignment is crucial for effective utilization of the latent space. On the other hand, the reconstruction term encourages the decoder to reconstruct the input data as accurately as possible, which encourages the decoder to accurately capture the data distribution.

While the expression of the ELBO might appear different from the original formulation, it remains equivalent, merely using different terms to express the same notion~\cite{Kingma_2019}.


Sampling from $q_{\phi}(z|x)$ enables the Monte Carlo estimation
\[ L_{\theta, \phi}(x) = - D_{KL}(q_{\phi}(z|x) || p(z)) + \frac{1}{L} \sum_{l=1}^{L} \log p_{\theta}(x|z^{(l)}) ,\]
where $z^{(l)} \sim q_{\phi}(z|x)$ and $L$ is the number of samples and first term is the Kullback-Leibler divergence term and the second term is the reconstruction term~\cite{Kingma_2019}. 


The individual data-point ELBO and i gradients are in general intractable to compute. However, unbiased estimates of the ELBO and its gradients can be obtained using the reparameterization trick, which is described in the next section~\cite{Kingma_2019}.


\subsection{The reparameterization Trick}

The reparameterization trick also  is a crucial component of VAEs. It is used to make the ELBO differentiable with respect to the parameters of the encoder $\phi$ and decoder $\theta$ through a change of variables~\cite{Kingma_2019}.

\subsubsection{Change of variables}

The notion is based on the fact that it is possible to express the random variable $z \sim q_{\phi}(z|x)$ as a differentiable function of a random variable $\epsilon$ and the parameters $\phi$ such that $z = g_{\phi}(\epsilon, x)$, where epsilon is a random variable that is independent of $\phi$ and $x$ and $\epsilon \sim p(\epsilon)$. Given this change of variables, the expectation with respect to $q_{\phi}(z|x)$ can be rewritten as an expectation with respect to $p(\epsilon)$
\[ E_{q_{\phi}(z|x)}[f(z)] = E_{p(\epsilon)}[f(g_{\phi}(\epsilon, x))], \]
where $f$ is an arbitrary function~\cite{Kingma_2019}.
As a result, the gradients the expectation and gradient operators become commutative, and there can be formed a Monte Carlo estimate of the gradients
\[ \nabla_{\phi} E_{q_{\phi}(z|x)}[f(z)] = \nabla_{\phi} E_{p(\epsilon)}[ f(g_{\phi}(\epsilon, x))] \]
\[ = E_{p(\epsilon)}[\nabla_{\phi} f(g_{\phi}(\epsilon, x))] \]
\[  \simeq \frac{1}{L} \sum_{l=1}^{L} \nabla_{\phi} f(g_{\phi}(\epsilon^{(l)}, x)) \]
where $\epsilon^{(l)} \sim p(\epsilon)$ and $L$ is the number of samples.
This is the reparameterization trick, which is further explained and illustrated in the figure~\ref{reparametrization}.

\begin{figure}
    \centering
    \input{figures/reparametrization.tex}
    \caption[Illustration diagram of the reparameterization trick]%
    {Illustration diagram of the reparameterization trick. The input of a function $f$ is $x$. The parameters $\theta$ affect the objective of the function $f$ through a random variable $z$. In the original form we can not compute the gradients $\nabla_{\phi} f$, because a direct backpropagation is not possible through a random variable. In the reparameterized form, the randomness is seperated from the parameters $\phi$, which enables the gradients to be computed. This is done by reparameterizing the random variable $z$ as a deterministic function and differentiable function  of $\phi$, $x$ and a new random variable $\epsilon$~\cite{Kingma_2019}}.

    \hspace*{15pt}\hbox{\scriptsize Credit: Adapted from Kingma and Welling\cite{Kingma_2019}  }\label{reparametrization}

\end{figure}

\subsubsection{Gradients of the ELBO}

When applying the reparameterization trick to the ELBO it becomes differentiable with respect to both $\phi$ and $\theta$, and it is possible to form a Monte Carlo estimate of the gradients
\[ \nabla_{\phi, \theta} L_{\theta, \phi}(x) = \nabla_{\phi, \theta} E_{q_{\phi}(z|x)} [\log p_{\theta}(x, z) - \log q_{\phi}(z|x)] \]
\[ = E_{p(\epsilon)}[\nabla_{\phi, \theta} [\log p_{\theta}(x, g_{\phi}(\epsilon, x)) - \log q_{\phi}(g_{\phi}(\epsilon, x)|x)]] \]
\[  \simeq \frac{1}{L} \sum_{l=1}^{L} \nabla_{\phi, \theta} [\log p_{\theta}(x, g_{\phi}(\epsilon^{(l)}, x)) - \log q_{\phi}(g_{\phi}(\epsilon^{(l)}, x)|x)] \]
where $\epsilon^{(l)} \sim p(\epsilon)$ and $L$ is the number of samples.

This is the key to training VAEs using stochastic gradient descent. The resulting Monte Carlo gradient estimate is used to update the parameters of the encoder and decoder networks~\cite{Kingma_2019}.


\subsection{Gaussian VAEs}

Although Gaussian VAEs are just a special case of VAEs, they are the most common type of VAEs. Gaussian VAEs assume that the prior distribution $p(z)$ is a centered Gaussian distribution $ p(z) = \mathcal{N}(0, I)$. They also assume that the decoder distribution $p_{\theta}(x|z)$ is a Gaussian distribution whose distribution parameters are computed from z with by the decoder network. The decoder distribution is given by
\[ p_{\theta}(x|z) = \mathcal{N}(f_{\theta}(z), I) \]
where $f_{\theta}(z)$ is the mean and $\sigma_{\theta}(z)$ is the standard deviation of the Gaussian distribution. Whilst there is a lot of freedom in the form $q_{\phi}(z|x)$ can take, Gaussian VAEs assume that $q_{\phi}(z|x)$ is also a Gaussian distribution with an approximately diagonal covariance matrix: 
\[ q_{\phi}(z|x) = \mathcal{N}(\mu_{\phi}(x), \sigma_{\phi}(x)) \]
where $\mu_{\phi}(x)$ and $\sigma_{\phi}(x)$ are the mean and standard deviation of the Gaussian distribution, which are computed by the encoder network.

To sample $z$ from $q_{\phi}(z|x)$, we can use the reparameterization trick described in the previous section
\[ z = \mu_{\phi}(x) + \sigma_{\phi}(x) \odot \epsilon, \] 
where $\epsilon \sim \mathcal{N}(0, I)$ is a random variable sampled from a standard Gaussian distribution and $\odot$ denotes element-wise multiplication.

When applying these assumptions to the ELBO, we get the following expression: \[ L_{\theta, \phi}(x) = - D_{KL}(q_{\phi}(z|x) || p(z)) + \frac{1}{L} \sum_{l=1}^{L} \log p_{\theta}(x|z^{(l)}) \]
\[ = - D_{KL}(\mathcal{N}(\mu_{\phi}(x), \sigma_{\phi}(x)) || \mathcal{N}(0, I)) + \frac{1}{L} \sum_{l=1}^{L} \log \mathcal{N}(x|f_{\theta}(z^{(l)}), I) \]
where $f_{\theta}(z^{(l)}) = f_{\theta}(\mu_{\phi}(x) + \sigma_{\phi}(x) \odot \epsilon^{(l)})$ and $\epsilon^{(l)} \sim \mathcal{N}(0, I)$.

However, the loss function to be \textbf{minimized} for VAE's usually used in practice is quite different from the ELBO negative. 
The function that is used in practice consists of: Mean Squared Error (MSE) reconstruction loss, KL divergence regularization loss and a constant $\beta$ that controls the importance of the regularization term
\[ L = \frac{1}{D} \sum_{i=1}^{D} ||x_i - \hat{x} ||^2 + \beta  \frac{1}{2} \sum_{i=1}^{Z} \biggl( -\log \sigma^2_\phi(x) - 1 + \mu^2_\phi(x) + \sigma^2_\phi(x) \biggr), \]
where $\hat{x} = f_{\theta}(\mu_{\phi}(x_i) + \sigma_{\phi}(x_i) \odot \epsilon^{(i)})$ and $\epsilon^{(i)} \sim \mathcal{N}(0, I)$, $D$ is the dimension of the input data and the $Z$ is the dimension of the latent space~\cite{Kingma_2019,betavae}. The second term in the function is derived from simplifying the KL divergence term in the ELBO, which is shown in the equation~\ref{eqKL}. The first term in the function is the MSE reconstruction loss because maximizing the Gaussian likelihood is approximately equivalent to minimizing the MSE reconstruction loss. This is shown in the equation~\ref{eqMSE}.
\begin{equation} \label{eqKL}
    \begin{split}
        D_{KL}(q_\phi(z|x) \| p_\theta(z)) &= \int q_\phi(z|x) \biggl[\ \log q_\phi(z|x) - \log p_\theta(z) \ \biggr] dz \\
        &= \int q_\phi(z|x) \biggl[\ -\frac{1}{2} \log (2\pi\sigma^2_\phi(x)) - \frac{(z - \mu_\phi(x))^2}{2\sigma^2_\phi(x)} \\
        &\qquad\qquad\qquad - \left( -\frac{1}{2} \log 2\pi - \frac{z^2}{2} \right) \ \biggr] \\
        &= \frac{1}{2} \int q_\phi(z|x) \biggl[ -\log \sigma^2_\phi(x) - \frac{(z - \mu_\phi(x))^2}{\sigma^2_\phi(x)} + z^2 \biggr] \\
        &= \frac{1}{2} \biggl( -\log \sigma^2_\phi(x) - 1 + \mu^2_\phi(x) + \sigma^2_\phi(x) \biggr)
    \end{split}
\end{equation}
\begin{equation} \label{eqMSE}
    \begin{split}
        \argmax_{\theta} \log \mathcal{N}(x|f_{\theta}(z), I) & = \argmax_{\theta} \log \biggl[\frac{1}{\sigma \sqrt{2\pi}} \exp \biggl( -\frac{1}{2 \sigma^2} (x - f_{\theta}(z))^2 \biggr)\biggr] \\
        & = \argmax_{\theta} \biggl[ \log \frac{1}{\sigma \sqrt{2\pi}} - \frac{1}{2 \sigma^2} (x - f_{\theta}(z))^2 \biggl]\\
        & = \argmax_{\theta} -\frac{1}{2} (x - f_{\theta}(z))^2
    \end{split}
\end{equation}

In the figure below~\ref{VAEFigure} there is a visualization of the architecture of Gaussian VAEs.

\begin{figure}[H]
    \centering 
    \input{figures/vae_tikz.tex}
    \caption[Architecture of Gaussian VAEs.]%
    { Architecture of Gaussian VAEs. The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. The sampled $z$ is then passed through the decoder with parameters $\theta$ producing the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss. 
    }
  	\medskip 
	\hspace*{15pt}\hbox{\scriptsize Credit: Adapted from Kingma and Welling~\cite{Kingma_2019} }\label{VAEFigure}
\end{figure}

\section{Vector Quantized VAEs}
\label{background:vqvae}

Vector Quantized VAEs (VQ-VAEs) are a variant of VAEs that were introduced in 2017 by Aäron van den Oord et al.~\cite{vqvae}. The VQ-VAEs have shown various improvements over the standard VAEs, such as higher quality of the generated sampled, better disentanglement of the latent space and better generalization to unseen data. The VQ-VAEs have been used in various applications, such as image generation, speech synthesis and music generation, text to image generation~\cite{vqvae2,vqvaespeechsynthesis, musicvqvae,dalle}.

The VQVAEs fundamentally differs in two key ways from a VAEs. Firstly, the latent representation is discrete instead of continuous. Secondly, the prior distribution is learned rather than being fixed. The posterior and prior distributions are categorical and the samples taken from these distributions are the indices of the embeddings in the embedding space. These matched indices are then used to look up the embeddings in the embedding space and then used as input to the decoder~\cite{vqvae}. VQ-VAEs learning
process consists of two stages. In the first stage, the encoder and the decoder are trained. In the second stage prior over these discrete latent variables is trained~\cite{vqvae}.

\subsection{Discrete Latent Variables}

VQ-VAEs focus on discrete latent variables, which is a more natural fit for many types of data. Language and speech naturally is stream of discrete units, such as words or phonemes. Images can be often well described by language, which can the discrete representations well suited for images as well. Moreover, discrete representations work very well with complex reasoning, decision-making~\cite{vqvae}.

VQ-VAEs define a latent embedding space $ e \in \mathbb{R}^{K \times D} $, where $K$ is the number of embeddings and $D$ is the dimension of each latent embedding vector. The model takes an input $x$, which is passed through the encoder producing output $z_e(x)$, as shown in figure~\ref{VQVAEFigure}. 
The discrete latent variables $z$ are then calculated by nearest neighbor lookup in the embedding space
 \[ z = \argmin_{k} || z_e(x) - e_k ||^2,\] 
where $e_k$ is the $k$-th embedding vector in the embedding space. The decoder then takes the discrete latent variables $z$ and produces the output $\hat{x}$. 
One can see this forward propagation as a regular autoencoder with a quantization step in the middle~\cite{vqvae}.

The posterior categorical distribution $q_{\phi}(z|x)$ is defined as follows:
\begin{equation} \label{eqVQVAEPosterior}
    q(z=k|x) = \begin{cases}
        1& \text{if} \ k = \argmin_{k} || z_e(x) - e_k ||^2 \\
        0& \text{otherwise}
    \end{cases},
\end{equation}
where $z_e(x)$ is the output of the encoder network and $e_k$ is the $k$-th vector in the embedding table.
The discrete latent variable $z$ is then used to look up the corresponding embedding vector $e_k$ in the embedding space, which is then used as input to the decoder network. The decoder network then produces the output $\hat{x}$~\cite{vqvae}.
The decoder distribution $p_{\theta}(x|z)$ is assumed to be a Gaussian distribution.

\subsection{Learning}

As mentioned earlier, the VQ-VAEs introduce learning the prior distribution separately from the posterior distribution. The prior distribution is defined as a categorical distribution $p_{\omega}(z)$, where $z$ is a discrete latent variable~\cite{vqvae}.

Since the proposed posterior distribution $q_{\phi}(z|x)$ is a deterministic by applying it to ELBO objective, we get the following expression:
\begin{equation}
    \begin{split}
        L_{\theta, \phi, \omega}(x) &= - D_{KL}(q_{\phi}(z = k|x) || p_{\omega}(z)) + \mathbb{E}_{q_{\phi}(z=k|x)} [\log p_{\theta}(x|z = k)],\\
                            &= - \mathbb{E}_{q_{\phi}(z=k|x)} [\log \frac{q_{\phi}(z=k|x)}{p_{\omega}(z)}] + \mathbb{E}_{q_{\phi}(z=k|x)} [\log p_{\theta}(x|z = k)],\\
                            &= - \log \frac{1}{p_{\omega}(z)} + \log p_{\theta}(x|z = k),\\
                            &= \log p_{\omega}(z) + \log p_{\theta}(x|z = k),\\
    \end{split}
\end{equation}

The VQVAE learning process is then divided into two stages, where in the first stage the first term is ignored, and the second term is maximized. In the second stage, the prior distribution is trained. In the next 2 sections, I will describe both stages in more detail.

\subsubsection{First stage}

In the first stage the log likelihood of posterior distribution is \textbf{maximized}, which means the encoder and the decoder are trained with the prior distribution being arbitrary. The training objective in first stages reduces to 
\[ L_{\theta, \phi}(x) = \log p_{\theta}(x|z = k), \]
where $k$ is the index of the nearest embedding vector in the embedding space, which is defined in equation~\ref{eqVQVAEPosterior}. We can look at the first stage as training a regular autoencoder with a quantization step in the middle, which inherently makes the latent space distribution categorical~\cite{vqvae}.
    
However, the expression $k = \argmin_{k} || z_e(x) - e_k ||^2$ is not differentiable with respect to the parameters of the network. To make the training process differentiable, the authors of the VQ-VAEs propose to use the straight-through estimator, which is a way of estimating the gradients of the non-differentiable function, and copy the gradients of $z_q(x)$ to $z_e(x)$~\cite{vqvae}. The straight through estimator only works if the difference between $z_e(x)$ and $e_k$ is small, which can be achieved by adding extra loss terms to the training objective.~\cite{straight_through}

This is where the VQ objective comes in. The VQ objective uses the second term of equation~\ref{eqVQVAEObjective} to encourage the encoder to produce representations that are close to the embedding vectors in the embedding space, which is called the commitment loss~\cite{vqvae}.

However, since the embedding space can be arbitrary large the embedding vectors can be arbitrary far from the encoder output. To prevent this, the authors of the VQ-VAEs propose to add another term to the training objective, which is called the codebook loss. The codebook loss encourages the embedding vectors to be close to the encoder output. The codebook loss has $\beta$ as a hyperparameter, which controls the importance of the codebook loss~\cite{vqvae}.

Thus, the resulting training objective becomes
\begin{equation} \label{eqVQVAEObjective}
    \begin{split}
        L &= \log p_{\theta}(x|z = k) - \biggl( || sg(z_e(x)) - e_k ||^2 + \beta || z_q(x) - sg(e_k) ||^2 \biggr),\\
    \end{split}
\end{equation}
where $sg$ is the stop gradient operation, which is defined as identity function, but with the gradients of the output set to zero.

The loss function to be \textbf{minimized} for VQ-VAE's usually used in practice is the sum of the VQ objective and the MSE reconstruction loss.
The first term in the function is the MSE reconstruction loss because maximizing the Gaussian likelihood is approximately equivalent to minimizing the MSE reconstruction loss. This is shown in the equation~\ref{eqMSE}. 
Thus, the resulting training objective to be minimized becomes
\[ L = \frac{1}{D} \sum_{i=1}^{D} ||x_i - \hat{x} ||^2 + || sg(z_e(x)) - e_k ||^2 + \beta || z_q(x) - sg(e_k) ||^2 , \]
where $\hat{x} = f_{\theta}(z_q(x))$, where function $f_{\theta}$ is the decoder network, $D$ is the dimension of the input data and $k$ is the index of the nearest embedding vector in the embedding space, which is defined in equation~\ref{eqVQVAEPosterior}. 

In the figure below~\ref{VQVAEFigure} there is a visualization of the architecture of VQ-VAEs.

\begin{figure}
    \centering 
    \input{figures/vq_vae.tex}
    \caption[Architecture of VQ-VAEs.]%
    { 
        Architecture of VQ-VAEs. The input $x$ is passed through the encoder convolutional neural network producing the output $z_e(x)$. For each output vector in $z_e(x)$, the nearest embedding vector in the embedding table $e$ is found. The indices of the nearest embedding vectors are then used as the discrete latent variables $z$. The discrete latent variables $z$ are then used to look up and retrieve the corresponding embedding vectors. The retrieved embedding vectors are then used as input to the decoder convolutional neural network producing the output $\hat{x}$. During the backward pass the gradients of the gradients of $z_q(x)$ are copied to $z_e(x)$ using the straight-through estimator, which is illustrated with a red arrow.
        Upper Left: The visualization of the embedding space during training.  The encoder output vector is shown as a dark green dot and the nearest embedding vector is shown as a dark purple dot. The commitment and codebook loss encourage the both the encoder output vector and the nearest embedding vector to be close to each other~\cite{vqvae}.
    }
  	\medskip 
	\hspace*{15pt}\hbox{\scriptsize Credit: Adapted from Aäron van den Oord et al.~\cite{vqvae}.}\label{VQVAEFigure}
\end{figure}

\subsubsection{Second stage}

The second stage objective is to train the prior distribution $p_{\omega}(z)$ over the discrete latent variables. The latent variables $z$ are sampled from the posterior distribution $q_{\phi}(z|x)$, which is defined in equation~\ref{eqVQVAEPosterior}. The prior distribution is categorical and can be made autoregressive by depending on other latent variables $z$~\cite{vqvae}.

The prior distribution $p_{\omega}(z)$ is then trained to match the distribution of the latent variables $z$ sampled from the posterior distribution $q_{\phi}(z|x)$. To achieve this the authors of the VQ-VAEs use an autoregressive model to model the prior distribution. The autoregressive model authors used is a Gated PixelCNN, which is a variant of PixelCNN, which is described in the next section~\cite{vqvae}.

\subsection{PixelCNNs}

The PixelCNNs are a prominent autoregressive architecture used in the field of pixel-level prediction. These models operate on images at the level of each individual pixels, learning to generate images or predict missing pixels one at a time. Deep autoregressive models have been shown to be very effective at modeling the full distribution and generating relatively low-resolution images. Generating high-resolution images with merely autoregressive models is challenging, because the size of the network increases rapidly with the size of the image~\cite{pixelcnn, pixelrnn}.

Autoregressive models treat and image as a sequence of pixels and the goal is to model the conditional distribution of each pixel given the previous pixels.
Image $x$ is represented as a one-dimensional sequence of pixels $x = (x_1, x_2, \dots, x_N)$, where $x_i$ is the $i$-th pixel in the image and $N$ is the number of pixels in the image. The estimate of the joint distribution of the pixels over an image $x$ is given by the product of the conditional distributions of each pixel given the previous pixels
\[ p(x) = \prod_{i=1}^{N} p(x_i|x_1, x_2, \dots, x_{i-1}),\]
where $p(x_i|x_1, x_2, \dots, x_{i-1})$ is the conditional distribution of the $i$-th pixel given the previous pixels. The generational process of the image is then done by sampling each pixel sequentially from the conditional distribution of the pixel given the previous pixels, which is shown in the figure~\ref{PixelCNNFigure}~\cite{pixelcnn}.


\begin{figure}
    \centering 
    \includegraphics[scale=0.20]{figures/pixelcnn.png}
    \caption[Conditional generation in autoregressive models]%
    {Conditional generation in autoregressive models. The model generates the pixels of the image one at a time, conditioning on the previous pixels. The model is autoregressive, because the distribution of each pixel is conditioned on the previous pixels.}
  	\medskip 
    \hspace*{15pt}\hbox{\scriptsize Credit: Adapted from the original PixelCNN paper~\cite{pixelcnn}.}\label{PixelCNNFigure}
\end{figure}

The PixelCNN in the original architecture is a stack of fifteen fully convolutional network layers with masked convolutions. The masked convolutions are used to ensure that the model can only access the previous pixels, which is crucial for model to be autoregressive. The model is trained to minimize the negative log likelihood of the training data. The PixelCNNs have been shown to be very effective at capturing the distribution of the data. Most current state-of-the-art models use the PixelCNNs as a building block for example PixelCNN++ and PixelSNAIL~\cite{pixelcnn, pixelcnnpp,pixelsnail}.

\subsubsection{Application in VQ-VAEs}

The PixelCNNs are used in the VQ-VAEs to model the prior distribution $p_{\omega}(z)$ over the discrete latent variables. The prior distribution is trained to match the distribution of the latent variables $z$ sampled from the posterior distribution $q_{\phi}(z|x)$. The PixelCNN in combination with the VQ-VAEs has been shown to be exceptional at capturing the distribution of the latent variables and generating high resolution samples~\cite{vqvae}.

\section{Semi-Conditioned VAEs}

Semi-conditional VAEs were first introduced in 2020~\cite{Gundersen_2021}. Semi-conditional VAEs (SCVAEs) are a variant of VAEs that first designed for the reconstruction of non-linear dynamic processes based on sparse observations. The semi-conditional VAEs extend the standard VAEs framework by adding an additional input $m$ to the decoder network $p_{\theta}(x|z, m)$, which is used to condition the decoder on the additional information. 

The additional information $m$ can be any type of information that is available at the time of the reconstruction and can be used to improve the reconstruction of the data. In the original paper, the authors used the SCVAEs to reconstruct fluid flow data. The additional information $m$ was the sparse measurements of the flow data. The method was showcased on two different datasets, velocity data from simulations of 2D flow around a cylinder, and bottom currents~\cite{Gundersen_2021}.

Natural applications for the SCVAEs are related to domains where there is often sparse measurements, such as environmental data. However, the SCVAEs can also be used, for instance, in computer vision to generate new image based on sparse pixel representations~\cite{Gundersen_2021}.

The semi-conditional property of the SCVAEs could also be applied to the VQ-VAEs, which has not been explored yet in the literature. Also, the potential of combining non-conditioned and semi-conditioned VAEs through multitask learning has not been explored yet.

\section{Multitask Learning}

Multitask learning is a machine learning paradigm where multiple tasks are learned at the same time, which has the aim of leveraging the shared information between the tasks to improve the performance of the individual tasks. Unlike the traditional single task learning, where each task is learned independently, multitask learning allows taking advantage of task relationships and to learn a shared representation that is useful for all tasks~\cite{multitasklearning}.

The notion of using multitask learning comes from the observation that the tasks are often related and depend on the same underlying features. This can be beneficial is scenarios where the data is limited, which is often the case for medical data~\cite{medicalMultiTask}.

For example, in medical image-analysis, in a scenario where the task is to develop a system for identifying and classifying different types of abnormalities in medical images. Traditionally this could be done by training a seperate model for each type of abnormality. However, this approach is not optimal due to limited data and the shared features that could be used to identify and classify multiple types of abnormalities~\cite{multitasklearning}.

Multitask learning is an approach in which a single model is trained to perform multiple tasks. In the context of medical image-analysis, this could involve training multitask VAE or VQVAE model capable simultaneously learning to reconstruct images, whilst also learning to classify different types of conditions. 

Multitask learning has been shown to be most effective when the tasks are related and data is limited. The potential and flexibility of multitask learning for improving the model performance make it a very valuable technique in machine learning. It has an unexplored potential in the context of VAEs and VQ-VAEs, which is the main motivation for this thesis.








