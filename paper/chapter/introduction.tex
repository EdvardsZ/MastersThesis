\chapter{Introduction}

This chapter provides an overview of the research topic and the underlying motivation for research in this thesis. In the first section, I will talk about the motivation behind the research. In the second section, I will talk about the objective of the thesis and the research questions that I will attempt to answer. Finally, I will provide a brief outline for the rest of the thesis.

\section{Motivation}

Over the past 10 years, Variational Autoencoders (VAEs) have become valuable assets in the field of deep learning and generative modeling. At the most basic level, they are used as tools for data generation and compression by learning the underlying patterns and structures present in a given dataset. VAEs have shown their versatility and have found applications in multiple domains, including image generation, anomaly detection, natural language processing, and speech synthesis~\cite{kingma2013autoencoding, Kingma_2019, vqvae, dalle}. However, VAEs, given a limited set of data and compute power, have limitations and challenges, which are described below.

One of the key challenges traditional VAEs face is that they tend to produce blurry, over-smoothed samples. This is due to the fact that the VAE objective is to maximize the evidence lower bound (ELBO) which is a trade-off between the reconstruction accuracy and the KL divergence between the approximate posterior and the prior. Also, the prior distribution is often chosen to be a simple distribution such as a standard normal distribution, which can limit the expressiveness of the latent space and the generative capabilities of the model~\cite{Kingma_2019}.

An additional challenge for traditional VAEs is the phenomenon known as posterior collapse. Posterior collapse occurs when the latent variables become uninformative, leading to the model ignoring them and relying solely on the decoder to reconstruct the input data. As a result, the model representations become less interpretable and the generative capabilities of the model are compromised~\cite{Kingma_2019}.

More recently Vector Quantized Variational Autoencoders (VQ-VAEs) have been introduced to address VAE's limitations. There are two key differences between VQ-VAEs and VAEs. One is that the latent space of VQ-VAEs is discrete, which is achieved by Vector Quantization (VQ). The other is that the prior distribution is learned instead of being assumed to be static. This allows the model to capture the underlying structure of the data more effectively, which allows VQ-VAEs to capture fine-grained details in the data, whilst maintaining the interpretability of latent representations and generational properties of VAEs. The prior distribution is learned by training a separate model called PixelCNN, which is trained to model the prior distribution of the latent space and thus achieve the ELBO objective with two models instead of one. VQ-VAEs have been shown to produce samples with higher fidelity and have been since used in a variety of applications such as high-resolution image generation, text-to-image generation and speech synthesis~\cite{vqvae,vqvae2, dalle}.

The VQ-VAE architecture solves the problem of capturing fine-grained details in the data, whilst maintaining the interpretability of latent representations and generational properties of VAEs. 
Although VQ-VAEs are more effective in capturing fine-grained details in the data, it is still challenging to capture all the sources of variation in the data in a single model. This is especially true when the data has multiple sources of variation, such as in the case of images, where the data can have multiple types of objects, backgrounds, and lighting conditions~\cite{Kingma_2019,vqvae, vqvae2}. One potential approach to address this issue could involve employing multitask learning, where the model is trained to tackle numerous tasks simultaneously.

Semi-conditional Variational Autoencoders (SC-VAEs) are a type of VAE in which the decoder distribution is conditioned on a specific subset of the input data. While SC-VAEs sacrifice the ability to efficiently generate samples solely from the latent space, they retain the capability to generate samples based on the provided subset of input data. This additional information is then used to reconstruct the input data, which has been shown to be useful in the context of reconstructing very sparse data and uncertainty quantification and looks to be a promising approach to be explored in the context of multitask learning~\cite{Gundersen_2021}.

Multitask learning is a paradigm that aims to improve model generalization and performance by simultaneously learning multiple related tasks. The concept of combining VAEs with multitask learning has undergone some experimental exploration in the research community, although it has not been thoroughly investigated~\cite{multitaskvib}. Multitask Variational Autoencoders (MT-VAEs) extend the VAE model to take advantage of the extra information or tasks that are available in the data. This approach holds the promise of improving the reconstruction accuracy and generalization capabilities of VAEs, as well as enhancing their interpretability~\cite{multitasklearning}.

The research in this thesis aims to investigate the effectiveness and potential of integrating semi-conditional and standard VAEs within a single model through multitask learning. The investigation will explore the potential of this approach in the context of both standard VAEs and VQ-VAEs. Specifically, the research will introduce two methods for combining semi-conditional and non-conditioned VAEs namely \methodOne{1} and \methodTwo{2}.

In the \methodOne{1}, the semi-conditional and non-conditioned VAEs are combined with two decoders, where one decoder is conditioned, and the other is not. In \methodTwo{2}, the semi-conditional and non-conditioned VAEs are combined with a single decoder, which can be conditioned or non-conditioned by masking the input data.

The main goal of this research is to investigate the potential of these methods to improve the reconstruction accuracy and generalization capabilities of VAEs and VQ-VAEs and by performing a thorough analysis and experimentation, the aim is to shed light on the advantages and limitations of these methods.

\section{Objective}

In this thesis, I aim to investigate and explore the integration of SCVAEs that can be combined with standard non-conditioned VAEs through multitask learning. My objective is to determine whether my proposed methods: \methodOne{1} and \methodTwo{2} which leverage multitask learning can enhance the performance of VAEs compared to traditional ones, and if so, under what conditions. Additionally, I will explore the applicability of these methods to VQ-VAEs.

More specifically, I will attempt to answer the following questions:

\begin{itemize}
    \item How can the \methodOne{1} and the \methodTwo{2} be implemented and integrated with Gaussian VAEs?
    \item Can the combination of semi-conditional and non-conditional VAEs be used to improve the reconstruction accuracy and generalization capabilities of VAEs on non-conditioned tasks?
    \item Could the same approaches be applied to VQ-VAEs?
    \item What are the limitations of these methods?

\end{itemize}

\section{Thesis Outline}

The outline for the rest of the thesis is as follows:

\begin{itemize}
    \item \textbf{Chapter 2: Background} - This chapter provides the necessary background information to understand the research presented in this thesis. 
    \item \textbf{Chapter 3: Methods} - This chapter describes the proposed methods for combining semi-conditional and standard non-conditional VAEs.
    \item \textbf{Chapter 4: Results} - This chapter presents the results of the experiments that were conducted to evaluate the proposed methods.
    \item \textbf{Chapter 5: Discussion} - This chapter discusses the results of the experiments and provides an analysis of the findings. Furthermore, it refers to the objectives and research questions of the thesis and provides an overview of the limitations and future work.
    \item \textbf{Chapter 6: Conclusion} - This chapter summarizes the research presented in this thesis and provides suggestions for future work.
    
\end{itemize}

