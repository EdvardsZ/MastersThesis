\chapter{Results}

This chapter presents the findings of the experiments conducted in this master's thesis. The chapter is divided into two sections.
The first section presents the results of \methodOne{1}, and the second section presents the results of \methodTwo{2}, where both methods will be evaluated in the context of both Gaussian VAEs and VQ-VAEs. In the final section, I will present the cross-validation results of both methods on all datasets and configurations in this thesis.

\section{Results of \methodOne{1}}

In this section, I will present the results of applying \methodOne{1} on both Gaussian VAEs and VQ-VAEs, with a comparative analysis of the baseline models without the method applied. The primary performance metrics that we will focus on are the reconstruction loss and the KL divergence loss of the latent space in the case of Gaussian VAEs and the VQ objective loss in the case of VQ-VAEs. 

The results presented in this section are based on the experiments conducted on the CelebA dataset with Config. 2 for Gaussian VAEs and Config. 3 for VQ-VAEs. The results on other configurations showed similar results with minor differences and can be found in the last section of this chapter~\ref{sec:cross_val_results}. The experiments will be compared and analyzed with respect to both Exact Sampling and Uniform Sampling and tested with the SoftAdapt loss balancing technique and without it. 

\subsection{Results on Gaussian VAEs}

The experiments showed that Gaussian VAEs \methodOne{1} was successful in its core task to train 2 decoders with a shared encoder. Upon examining the results, it was observed that the initial idea that this method would reduce the reconstruction loss of both the conditioned and non-conditioned decoders was confirmed. However, the KL divergence loss of the latent space increased with the application of \methodOne{1}, which can be seen in figure \ref{fig:results_method1_gaussian_vae} and table \ref{tab:results_method1_gaussian_vae}.

The conditioned decoder produced outputs with much higher quality when compared to the non-conditioned decoder. This can be observed in figure~\ref{fig:rec_gaussian}. When comparing the results of the Exact Sampling and Uniform Sampling, in this case, the analysis revealed little difference between the two, which can be seen in table \ref{tab:results_method1_gaussian_vae}.

The experiments were also run with the SoftAdapt loss balancing technique and without it. In this case, the results showed
that there was little to no difference between the two. However, the training validation loss looked more stable when using SoftAdapt, with fewer fluctuations in the losses.

The experiments were also run with deeper neural networks, which showed that training instability in the form of posterior collapse was more likely to happen. This is a common problem in Gaussian VAEs~\cite{wang2023posterior}. That is why a more sophisticated configuration of a Gaussian VAE was not used in the experiments. 

% TODO: Add this to the discussion
% This can be explained by the fact that there is a trade-off between the quality of the reconstruction and the KL divergence of the latent space. %

\begin{table}[H]
    \centering
    \input{figures/tables/scvae2d.tex}
    \caption{Cross-validation results of \methodOne{1} applied to a Gaussian VAE(Config. 2) on the CelebA dataset.}
    \label{tab:results_method1_gaussian_vae}
\end{table}

\begin{figure}[H]
    \centering
    \scalebox{0.48}{\input{figures/results/SCVAE2D_EXACT/kl_loss.pgf}}
    \scalebox{0.48}{\input{figures/results/SCVAE2D_EXACT/recon_loss.pgf}}
    \caption[Validation loss during training of a Gaussian VAE.]
    {
        (Average) Validation losses during training with and without \methodOne{1} applied on Gaussian VAE(Config. 2). The results are shown for the non-conditioned decoder - $Decoder_1$, and Exact Sampling. The dataset used is CelebA.
        Left: KL divergence loss of the latent space comparison. Right: Reconstruction loss comparison of the $Decoder_1$ - non-conditioned decoder.
    }
    \label{fig:results_method1_gaussian_vae}
\end{figure}


\begin{figure}[H]
    \centering
    \input{figures/results/scvae2d.tex}
    \caption[Trained neural network with \methodOne{1} applied to a Gaussian VAE.]
    { 
        Outputs of a trained Gaussian VAE with \methodOne{1} Exact Sampling applied to a Gaussian VAE(Config. 1) on CelebA dataset. The image from the conditioned decoder is reconstructed with higher quality compared to the non-conditioned decoder because the conditioned decoder $Decoder_2$ uses conditioning information $m$ to improve the quality of the reconstruction.
    }
    \label{fig:rec_gaussian}
\end{figure}

\subsection{Results on VQ-VAEs}

Overall, upon examining the results on VQ-VAEs, it was observed that when applying \methodOne{1} with default weight settings, the reconstruction loss was reduced for both the conditioned and non-conditioned decoders, which means an overall improvement in the quality of the reconstruction, which was observed for both sampling methods. However, the VQ objective loss increased when SoftAdapt was not used, which can be seen in Table \ref{tab:results_method1_vq_vae}.

Furthermore, incorporating SoftAdapt, which uses adaptive weight balancing, resulted in notable improvements, see section \ref{background:softadapt}. More specifically, both the reconstruction loss and the VQ objective loss decreased when SoftAdapt was utilized, as compared to the baseline configuration. This shows that the model's performance was enhanced with the application of our method and SoftAdapt technique, leading to an overall better-performing model, which can be seen in table \ref{tab:results_method1_vq_vae} and figure \ref{fig:results_method1_vq_vae}. 

When SoftAdapt was incorporated into the training, the training stability was improved, with fewer fluctuations in the loss. The results showed that the model was able to learn the optimal weights for the loss functions, which resulted in a more stable training process, which can be seen in figure \ref{fig:results_method1_vq_vae}.

The conditioned decoder was able to reconstruct the image with a higher quality compared to the non-conditioned decoder, however, the difference observed was not as significant as with Gaussian VAEs, which can also be seen in figure \ref{fig:rec_vqvae}. 

% Move to discussion %
%This could be explained by the fact that the VQ-VAEs are more robust and stable compared to Gaussian VAEs.
%The reasoning behind this lies in the fact that the VQ-VAEs already have a high-quality reconstruction, which makes it harder to improve the quality of the reconstruction.

\begin{table}[H]
    \centering
    \input{figures/tables/scvqvae2d.tex}
    \caption{Cross-validation results of \methodOne{1} applied to a VQ-VAE(Config. Nr. 3) on the CelebA dataset.}
    \label{tab:results_method1_vq_vae}
\end{table}

\begin{figure}[H]
    \centering
    \scalebox{0.48}{\input{figures/results/SCVQVAE2D_EXACT/vq_loss.pgf}}
    \scalebox{0.48}{\input{figures/results/SCVQVAE2D_EXACT/recon_loss.pgf}}
    \caption[Validation loss comparison during training of a Gaussian VAE.]
    {
        (Average) Validation losses during training with and without \methodOne{1} applied on VQ-VAE (Config. 3) and CelebA dataset. The results of \methodOne{1} in the plot shown for the non-conditioned decoder - $Decoder_1$, and Exact Sampling is used with SoftAdapt enabled.
        Left: VQ objective loss comparison. Right: Reconstruction loss comparison.
    }
    \label{fig:results_method1_vq_vae}
\end{figure}

\begin{figure}[H]
    \centering
    \input{figures/results/scvqvae2d.tex}
    \caption[Trained neural network with \methodOne{1} applied to a VQ-VAE.]
    {
        Outputs of a trained VQ-VAE with \methodOne{1} and Exact Sampling applied to a VQ-VAE (Config. 3). The image from the conditioned decoder is reconstructed with higher quality compared to the non-conditioned decoder because the conditioned decoder $Decoder_2$ uses the extra conditioning information $m$ and tries to improve the quality of the reconstruction.
    }
    \label{fig:rec_vqvae}
\end{figure}


\section{Results of \methodTwo{2}}

In this section, I will present the results of \methodTwo{2} on both Gaussian VAEs and VQ-VAEs, which will be compared against baseline models without the method applied. Although in the training process of \methodTwo{2}, the reconstruction loss that is minimized uses the conditioning information, the primary performance metrics that we will focus on and evaluate are the reconstruction loss in a non-conditioned setting and the KL divergence loss of the latent space in the case of Gaussian VAEs and the VQ objective loss in the case of VQ-VAEs.

The results presented in this section are based on the experiments conducted on the CelebA dataset with Config. 2 for Gaussian VAEs and Config. 3 for VQ-VAEs, however, the rest of the configurations showed similar results with minor differences and can be found in the last section of this chapter~\ref{sec:cross_val_results}. The analysis will be done with respect to both Uniform Sampling and Gaussian Sampling and tested with the Power law distribution with different exponent values.

% TODO: MENTION in the methodology the different exponent values for the power-law distribution

\subsection{Results on Gaussian VAEs}

The results indicated that \methodTwo{2} yielded promising outcomes with Gaussian VAEs. Upon examining the results, it was observed and noted the application of this method led to a substantial reduction in the KL divergence loss of the latent space. However, when no conditioning information was provided, there was an increase in the reconstruction loss compared to the baseline model. These findings can be seen in table \ref{tab:results_method2_gaussian_vae} and figure \ref{fig:results_method2_gaussian_vae}. This held for both Uniform and Gaussian sampling methods.

% ------------ This is important to create a graph for this ------------ %
In addition to this, experiments with a range of different exponent values for the power-law distribution were conducted. The findings showed that the higher the exponent value the more the model was able to reduce the reconstruction loss of the scenario, where no conditioning information is given. However, the KL divergence loss of the latent space increased with higher exponent values.
% TODO: Make a graph showing this !!!

%For Uniform sampling, the results showed that the KL divergence loss of the latent space was reduced more compared to Gaussian sampling. One possible explanation for this could be that with Gaussian sampling, there is a higher probability of sampling the same pixel multiple times, which can be less informative for the decoder.
% Move this to the discussion

\begin{table}[H]
    \centering
    \input{figures/tables/scvae1d.tex}
    \caption{Cross-validation results of \methodTwo{2} applied to a Gaussian VAE(Config. 2) on the CelebA dataset.}
    \label{tab:results_method2_gaussian_vae}
\end{table}

\begin{figure}[H]
    \centering
    \scalebox{0.48}{\input{figures/results/SCVAE1D_GAUSSIAN/kl_loss.pgf}}
    \scalebox{0.48}{\input{figures/results/SCVAE1D_GAUSSIAN/recon_loss.pgf}}
    \caption[Validation loss during training with \methodTwo{2} applied on Gaussian VAE.]
    {
        (Average) Validation losses during training with and without \methodTwo{2} applied on Gaussian VAE Configuration 2.
        Left: KL loss comparison. Right: Reconstruction loss comparison of when conditioning information is masked out.
    }
    \label{fig:results_method2_gaussian_vae}
\end{figure}

\subsection{Results on VQ-VAEs}

Upon examining the results on VQ-VAEs, the findings indicated that \methodTwo{2} worked very well with VQ-VAEs.The results showed it can be used to substantially reduce the VQ objective loss and at the same time to improve the quality of the reconstruction in the scenario where no conditioning information is given. 

This showed to be the case in many experiments with both Uniform and Gaussian sampling, with a high enough exponent value for the power-law distribution. However, the results showed that this method worked slightly better with Gaussian sampling compared to Uniform sampling, which can be seen in table \ref{tab:results_method2_vq_vae}.

% TODO make a graph showing the difference between exponent values

% TODO move to discussion
% This can be explained by the fact that the information around the center of the image is more important for the reconstruction, which is more likely to be sampled with Gaussian sampling.

\begin{table}[H]
    \centering
    \input{figures/tables/scvqvae1d.tex}
    \caption{Cross-validation results of \methodTwo{2} applied to a VQ-VAE(Config. Nr. 3) on the CelebA dataset.}
    \label{tab:results_method2_vq_vae}
\end{table}
\begin{figure}[H]
    \centering
    \scalebox{0.48}{\input{figures/results/SCVQVAE1D_GAUSSIAN/vq_loss.pgf}}
    \scalebox{0.48}{\input{figures/results/SCVQVAE1D_GAUSSIAN/recon_loss.pgf}}
    \caption[Validation loss comparison during training of a VQ-VAE.]
    {
        (Average) Validation losses during training with and without \methodTwo{2} applied on VQ-VAE Configuration 3.
        Left: VQ objective loss comparison.  Reconstruction loss comparison of when conditioning information is masked out.
    }
    \label{fig:results_method2_vq_vae}
\end{figure}

\section{Cross-validation results} \label{sec:cross_val_results}

This section presents the cross-validation results of both methods on all datasets and configurations in this thesis. The cross-validation was conducted with 5 folds, where the data was split into 80\% training and 20\% validation. The final cross-validation results are presented in tables \ref{tab:cross_val_results_celeba}, \ref{tab:cross_val_results_cifar10}, and \ref{tab:cross_val_results_mnist}.

% Create a table with description
\begin{table}[H]
    \input{figures/tables/CelebA.tex}
    \caption{Cross-validation results of \methodOne{1} and \methodTwo{2} on the CelebA dataset.}
    \label{tab:cross_val_results_celeba}
\end{table}
% Create a table with description
\begin{table}
    \input{figures/tables/CIFAR10.tex}
    \caption{Cross-validation results of \methodOne{1} and \methodTwo{2} on the CIFAR10 dataset.}
    \label{tab:cross_val_results_cifar10}
\end{table}

% Create a table with description
\begin{table}
    \input{figures/tables/MNIST.tex}
    \caption{Cross-validation results of \methodOne{1} and \methodTwo{2} on the MNIST dataset.}
    \label{tab:cross_val_results_mnist}
\end{table}