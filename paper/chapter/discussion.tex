\chapter{Discussion}

In this chapter, the results in the preceding chapter are analyzed, interpreted and discussed in the context of the objective and research questions. This chapter is divided into five sections. The first section discusses the results of \methodOne{1}, and the second section discusses the results of \methodTwo{2}. The third section provides a comparative analysis of the two methods, and the final two sections discuss the limitations of the research and potential future work.

\section{Analysis of \methodOne{1}}

The results of \methodOne{1} show that it is overall a viable method for combining semi-conditional and non-conditional VAEs. The resulting model has multitask capabilities, which allow it to improve the quality of the reconstruction and the generalization capabilities of the VAE. It was shown that the method can be applied to both VQ-VAE  and Gaussian VAEs.

One of the disadvantages of this method is that it requires a second decoder, which increases the complexity of the model and the computational cost. However, the results show that in some cases, the increase in complexity is justified by the improvement in the quality of the reconstruction.

\subsection{Findings on Gaussian VAEs}

The results on Gaussian VAEs showed that a standard Gaussian VAE can be combined with a semi-conditional VAE by using \methodOne{1}. This method improved the reconstruction quality of the non-conditioned decoder and added a second decoder that allows the model to reconstruct or generate images given some pixels. However, the results showed that this meant a slight increase in the KL divergence loss of the latent space. This is expected since the model has to learn two decoders instead of one, and it means it puts less emphasis on the KL divergence loss of the latent space. Consequently, the model has a trade-off between the KL divergence loss of the latent space and the reconstruction loss of both decoders.

It could be observed that the conditioned decoder was able to reconstruct the images with noticeably higher quality than the non-conditioned decoder. This is expected since the conditioned decoder has more direct information about image pixels, which makes it easier to reconstruct.  

When comparing Exact and Uniform sampling, I did not see any significant difference in the results. This is something that could be further investigated in future work. One possible explanation for this could be that the model is not able to fully take advantage of both sampling methods because it has already a trade-off between the KL divergence loss of the latent space and the reconstruction loss of both decoders.

\subsection{Findings on VQ-VAEs}

When applying \methodOne{1} to VQ-VAEs, the results showed that both the quality of the reconstruction and the VQ objective loss can be improved. As a result, the model gets multitask properties to reconstruct and generate images given some pixels or to reconstruct images with higher quality. The improvement can be explained that even though 2 decoders are used, they still share the same encoder, which can make it better at its core task. 

It was observed that the conditioned decoder was able to reconstruct the image with a higher quality compared to the non-conditioned decoder, however, the difference observed was not as significant as with Gaussian VAEs. The reasoning behind this could be that the VQ-VAEs already have a high-quality reconstruction, which makes it harder to improve the quality of the reconstruction.

The \methodOne{1} was tested with both Exact Sampling and Uniform Sampling, and it was observed even though the results were similar, the Uniform Sampling showed slightly better results. The rationale behind this could be that Uniform Sampling gives much more diverse samples, which can be more informative for the network.


\section{Analysis of \methodTwo{2}}

\methodTwo{2} involves using the same decoder to unify both the conditioned and non-conditioned tasks, which is done by using variable conditioning - a technique that allows conditioning of the decoder on a variable amount of information or just an empty mask. The rationale behind this is that the model decoder can learn to do both tasks and can detect if and where the mask is empty.

One of the advantages of this method is that it does not require a second decoder, which reduces the complexity of the model and the computational cost, which can be a problem for large-scale high-resolution models.

\subsection{Findings on Gaussian VAEs}

Upon analysis, it has become clear that when \methodTwo{2} is applied to Gaussian VAEs, the decoder of the model can reconstruct images given some pixels as conditioned information or with no information at all. This method improved substantially the KL divergence loss of the latent space, which means that it is possible to generate more accurate samples from the latent space. 

However, one of the drawbacks of this method is that it reduced the quality of the reconstruction in the non-conditioned case. The cause for this could be that the decoder and the encoder are not deep enough to learn to do both tasks at the same time. In my experiments, when using a deeper encoder and decoder, it resulted in posterior collapse, which is a common problem in Gaussian VAEs.

It could also be observed that Uniform Sampling showed better results than Gaussian Sampling. The cause for this could be traced back to the fact that for Gaussian Sampling, there's a high chance to sample the same pixel multiple times, which can make the model overfit to the same pixels.


\subsection{Findings on VQ-VAEs}

When applying \methodTwo{2} to VQ-VAEs, the results showed that the model had significantly reduced the VQ objective loss, which means that the model is more accurate a can be used more effectively for image generation. One possible explanation for the improvement in the VQ objective loss is that the conditioning of the decoder gives the encoder more direct and accurate gradients to learn from and this can be beneficial in the early stages of training when the embeddings are very sparse and the gradients are very noisy.

The experiments showed that the reconstruction quality in the case of no conditioning could also be improved if the exponent value of the Power Law distribution was set to a higher value. This is because the higher the exponent value of the Power Law distribution, the fewer pixels on average are sampled, which means that the model more often has to reconstruct the image from scratch.

% TODO check if this true %
It was discovered that the Gaussian Sampling showed slightly better results than Uniform Sampling. The reason behind this could be that the Gaussian Sampling more often samples the center pixels of the image, which are more informative for the network.

\section{Comparative Analysis}

In the context of comparing the two methods, one must first and foremost consider the complexity of the model. The \methodOne{1} requires a second decoder,
which increases the complexity of the model and the computational cost. This can be a problem for high-resolution and large-scale models, as it could mean that the model is too slow to train or too computationally expensive to use. On the other hand, \methodTwo{2} does not require a second decoder, which does not introduce a lot of extra complexity to the model. This makes it more suitable for real-world applications.

In both approaches applied to Gaussian VAEs with deeper encoder and decoder networks, the training instability in the form of the posterior collapse was observed, which means that latent variables have become uninformative, leading the model to ignore them~\cite{lucas2019dont}. This is a common problem in Gaussian VAEs and can be hard to avoid. However, this is something that is not a problem by design in VQ-VAEs, which makes them more stable and easier to train.

In both of the methods and implementations, it was decided to use a fully connected layer to use the conditioned information as input to the decoder. This was done to keep the model simple and avoid introducing too much complexity to the models. However, it could have been done differently, for example, by using convolutional layers to use the conditioned information as input to the decoder.

%Try to compare the results of the two methods
Both methods brought improvements to the quality of the reconstruction and the generalization capabilities when applied to VQ-VAEs. However, the \methodTwo{2} showed better results in terms of the VQ objective loss, while \methodOne{1} showed better results in terms of the reconstruction quality. This could be explained by the fact that \methodOne{1} has two decoders, which can make it better at the core task of the VQ-VAE, which is to reconstruct the image.


% Tell about the extra complexity of the model and how it can be a problem for large-scale models.
%One drawback of this method lies in the fact that it requires a second decoder, which increases the complexity of the model and the computational cost which can be a problem for large-scale models. This is something that was also observed in the experiments.
% Tell about the trickiness of the training in gaussian vaes and how it is easier in vq-vaes.
% Tell about the risk of posterior collapse in Gaussian VAEs and how it is not a problem in VQ-VAEs
% SoftAdapt overall?.

\section{Limitations}

One of the limitations of this research lies in the fact in the scope of experimentation. While the conducted experiments shed light on the capabilities and limitations of the methods, a broader range of hyperparameters, datasets and models could have been explored. This broader exploration could have provided a deeper understanding of the methods and their capabilities and limitations. Additionally, the experiments were conducted on relatively small-resolution images, which could have affected the results.

Furthermore, the comparative analysis of different sampling methods could have been more extensive. Expanding the range of experiments in this regard could have provided a better insight into the differences between the sampling methods and how they affect the results of the model.

Moreover, it's important to acknowledge the constraints posed by the computational resources. With access to more computational resources, it would have been possible to conduct more experiments with higher precision, potentially utilizing more complex models and utilizing more folds in cross-validation. Additionally, the increased computational capacity would have facilitated the exploration across a wider array of hyperparameters in the same time frame, enabling a more comprehensive investigation of the methods.

\section{Future Work}

One possible future direction for this research could be to investigate the possibility of employing a training schedule where the model is initially trained with \methodOne{1} and then later switched to a regular VQ-VAE or Gaussian VAE. This could potentially improve the results of the model for unconditional generation tasks.

As previously mentioned for both methods and implementations in this thesis it was decided to use a fully connected layer to employ the conditioned information as input to the decoder. A possible future direction could be to use convolutional layers to use the conditioned information as input to the decoder.

This research explored the possibilities of combining semi-conditional and non-conditional VAEs. However, this could also be for other generational models, such as GANs or diffusion models. Another possible application of this method could be to DALL-E architecture, which is a text-to-image generation model based on VQ-VAEs.