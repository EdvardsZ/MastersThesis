\chapter{Methods}

This chapter will outline the methods how SCVAEs and non-conditioned VAEs are integrated using multitask learning in this thesis. Initially, describe the method of merging SCVAEs with non-conditioned VAEs by expanding the SCVAEs framework with a second decoder. This method will be applied to both Gaussian VAEs and VQ-VAEs. In the following section, I will describe another method of merging SCVAEs with non-conditioned VAEs by utilizing the same encoder and decoder for both models by employing novel training strategies. This method will also be applied to both Gaussian VAEs and VQ-VAEs.

\section{Method 1}

The SCVAE1D method and its VQ-VAE counterpart SCVQVAE1D is 

\begin{figure}
    \centering 
    \input{figures/scvae2d.tex}
    \caption[Architecture of SCVAE1D.]%
    { 
        Architecture of Gaussian VAEs. The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. The sampled $z$ is then passed through the decoder with parameters $\theta$ producing the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss. 
    }
  	\medskip 
	\hspace*{15pt}\hbox{\scriptsize Credit: Adapted from AÃ¤ron van den Oord et al.~\cite{vqvae}.}\label{SCVAE1DFigure}
\end{figure}




