\chapter{Methods}

This chapter will outline the methods of how SCVAEs and non-conditioned VAEs are
integrated using multitask learning in this thesis, which will be explored in
the context of images. The chapter will be divided into three sections. The first
section will describe the conditioning strategies, which will be used to
condition the VAEs. In the following section, I will describe the method of
merging SCVAEs with non-conditioned VAEs using two decoders, where one is
conditioned, and the other is not. This method will be referred to as
\methodOne{1}. In the following section, I describe another method of merging
SCVAEs with non-conditioned VAEs utilizing the same encoder and decoder for
both models by employing novel training strategies, which will be referred to
as \methodTwo{2}. Both methods will be applied to both Gaussian VAEs and VQ-VAEs.

\section{Conditioning information}

In this thesis, I will obtain the conditioning information $m$ from the input
image $x$ by sampling pixels some pixels from the input image $x$. In this
process, the first step is to obtain a mask which will be used to sample
the pixels from the input image $x$. The mask will be a binary matrix of the
same size as the input image $x$, where a value of 1 represents the sampled
pixels and a value of 0 represents the pixels that are not sampled. The
decision of how many pixels to sample and which pixels to sample will be described
in the following sections for each method.

After the mask is obtained, the mask is applied to the input image $x$ to
obtain the masked image, which is then used to obtain the conditioning
information $m$ by concatenating the mask with the masked image, which can be
seen in Figure~\ref{ConditioningFigure}.

In both proposed methods, the conditioning information $m$ will be incorporated
into the decoder by first flattening it and then concatenating it with the input of the decoder and then using a fully connected layer before passing it through the transposed convolutional layers of the decoder.

\begin{figure}
    \centering
    \input{figures/conditioning.tex}
    \caption[The illustration of the process of obtaining the conditioning information $m$ from the input image $x$ and the mask.]%
    {
        The illustration of the process of obtaining the conditioning information $m$ from the input image $x$ and the mask. The process to obtain the conditioning information $m$ consists of two steps: the first step is to obtain the masked image from the input image $x$ and the mask, and the second step is to add the mask to the masked image to obtain the conditioning information $m$. The conditioning information $m$ is then used to condition the decoder.
    }\label{ConditioningFigure}
\end{figure}

\section{\methodOne{1}}

In the first method, the VAE framework is expanded with a second decoder
$p_\xi(x|z,m)$, which is conditioned on an additional property $m$ of the input
data. This approach involves utilizing two decoders within the VAE framework:
one for reconstructing the input data solely based on a trained neural network with \methodOne{1} latent variable $z$,
and the other for reconstructing the input data based on both the latent
variable $z$ and the additional conditioning information $m$ see Figures \ref{SCVAE2DFigure} and \ref{SCVQVAE2DFigure}.

\subsection{Conditioning strategy}

In this method, the conditioning information $m$ is acquired by sampling a
constant number of pixels from the input data. To sample the pixels from the
input image $x$, I will use a pixel sampling operation, which will be described
in the following section.

\subsubsection{Pixel Sampling operation}

I will explore two-pixel sampling types for this method,
which can be seen in Figure~\ref{SamplingFigure} and are described as follows:

\begin{enumerate}
    \item \textbf{Exact Sampling:} In this sampling type, the conditioning information $m$ is sampled from the input image $x$ by sampling the same sparse pixels from the input image $x$. In this work, the pixels that will be sampled from the input image will be a sparse grid of pixels.
    \item \textbf{Uniform Random Sampling:} In this sampling type, the conditioning information $m$ is sampled from the input image $x$ by sampling the exact number of pixels from the input image $x$ uniformly at random.
\end{enumerate}

The sampling process will be done every time the input image $x$ is passed through the model. This means that in the case of the Uniform Random Sampling, the conditioning information $m$ will be different for each time the image is passed through the model. This is done to ensure that the model learns to generalize and handle various cases.

\subsection{Application to Gaussian VAEs}

In Gaussian VAEs, the integration of a second conditioned decoder
$p_\xi(x|z,m)$ follows a similar approach as outlined in the general method
description. However, there are specific adjustments and considerations that
need to be taken into account when applying this method to Gaussian VAEs.

The integration of the second conditioned decoder $p_\xi(x|z,m)$ into the
Gaussian VAEs framework involves flattening on concatenating the latent
variable $z$ and the conditioning information $m$ and passing it through a
fully connected layer before passing it through the transposed convolutional
layers of the second decoder. Although a new latent variable could have
been sampled for the second decoder, the same latent variable $z$ is used
instead of sampling a new one. This is done to ensure that the latent variable
$z$ is the same for both decoders and ensures that it is easier to compare the
reconstructions of the input data. The architecture of the Gaussian
VAE framework with the second conditioned decoder is illustrated in
Figure~\ref{SCVAE2DFigure}

Although the training strategy could have been to train decoders by alternating between them, in this thesis, I will explore the training strategy where both decoders are trained simultaneously.

In Gaussian VAEs, the loss objective consists of two components: the
reconstruction loss and the KL divergence term. However, with the inclusion of
the second conditioned decoder, the loss objective is extended to include the
reconstruction loss of the second conditioned decoder. The overall loss
objective to be \textbf{minimized} becomes

\[ L = W_1 \frac{1}{D} \sum_{i=1}^{D} ||x_i - \hat{x}_{1_{i}} ||^2 + W_2 \frac{1}{D} \sum_{i=1}^{D} || x_i - \hat{x}_{2_{i}} ||^2 + \beta  \frac{1}{2} \sum_{i=1}^{Z} \biggl( -\log \sigma^2_\phi(x)_i - 1 + \mu^2_\phi(x)_i + \sigma^2_\phi(x)_i \biggr), \]

where $D$ is the number of pixels in the input image, $x_i$ is the $i$-th pixel
of the input image, $\hat{x}_1$ is the output of the first decoder, $\hat{x}_2$
is the output of the second decoder, $\beta$ is the KL divergence weight, Z is
the dimension of the latent space, $\mu_\phi(x)$ and $\sigma_\phi(x)$ are the
mean and the standard deviation of the Gaussian distribution produced by the
encoder, respectively. The resulting loss has also extra weight parameters $W_1$ and $W_2$ to balance and control the importance of reconstruction loss, which will be hyperparameters of the model.

During the training process, the encoder is trained to produce an accurate mean
and standard deviation of the Gaussian distribution, and the decoders are
trained to produce accurate reconstructions of the input data.

As a result of the integration of the second conditioned decoder, the model has
the ability to reconstruct the input data based just on the latent variable $z$
and as well as the conditioning information $m$.
\begin{figure}[H]
    \centering
    \input{figures/scvae2d.tex}
    \caption[\methodOne{1} applied to Gaussian VAEs.]%
    {
        \methodOne\ applied to Gaussian VAEs. The Gaussian VAEs framework is extended by adding a second conditioned decoder $p_\xi(x|z,m)$. The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. The sampled $z$ is then used as input to both decoders. As a result, the loss function consists of three components: the MSE reconstruction loss of the first decoder, the MSE reconstruction loss of the second decoder and the KL divergence regularization loss.
    }\label{SCVAE2DFigure}
\end{figure}

\subsection{Application to VQ-VAEs}

The approach of integrating the second conditioned decoder $p_\xi(x|z,m)$ into
the VQ-VAEs framework is similar to the approach used for Gaussian VAEs.
However, some differences need to be taken into account when applying
this method to VQ-VAEs.

One of the main differences is that the VQ-VAEs framework uses a discrete
latent space and the Vector Quantization which is described in \autoref{background:vqvae}. This means that the latent variable $z$ is a discrete variable that represents the indexes of the embedding table vectors. This means that first the corresponding embedding table vectors $z_q(x)$ must be computed for the latent variable $z$ before flattening and concatenating it with the conditioning information $m$ and then using a fully connected layer before the transposed convolutional layers of the second decoder. The architecture of the VQ-VAEs framework with the second conditioned decoder is illustrated in Figure~\ref{SCVQVAE2DFigure}.

The training strategy is the same as in Gaussian VAEs, where both decoders are trained simultaneously. This means that the loss objective must be extended to include the reconstruction loss of the second conditioned decoder. In VQ-VAEs, the loss objective consists of three components: the MSE
reconstruction loss the commitment loss and the codebook loss. However,
with the inclusion of the second conditioned decoder, the loss objective is
extended to include the reconstruction loss of the second conditioned decoder.
The overall loss objective to be \textbf{minimized} becomes

\[ L = W_1 \frac{1}{D} \sum_{i=1}^{D} ||x_i - \hat{x}_{1_{i}} ||^2 +  W_2 \frac{1}{D} \sum_{i=1}^{D} || x_i - \hat{x}_{2_{i}} ||^2 + \frac{1}{Z} \sum_{i=1}^{Z} \biggl( || sg(z_e(x)_i) - e_{k_i} ||^2 + \beta || z_q(x)_i - sg(e_{k{_i}}) ||^2 \biggr) , \]

% TODO explain the weights W1 and W2

where $D$ is the number of pixels in the input image, $x_i$ is the $i$-th pixel
of the input image, $\hat{x}_1$ is the output of the first decoder, $\hat{x}_2$
is the output of the second decoder, $Z$ is the number of latent space vectors,
$z_e(x)$ is the output of the encoder, $z_q(x)$ is the mapping output after
Vector Quantization, $e_k$ is the $k$-th embedding table vector, $\beta$ is the
commitment loss weight, $sg$ is the stop gradient operation, which was
previously defined in the background \autoref{background:vqvae}.

After the first stage of the training, the model has the ability to reconstruct
the input data based just on the latent variable $z$ and as well as the
conditioning information $m$. To generate the latent variable $z$, one must be
trained a separate autoregressive model, which will be used to generate the
latent variable $z$.

\begin{figure}[H]
    \centering
    \input{figures/scvqvae2d.tex}
    \caption[\methodOne{1} applied to VQ-VAEs.]%
    {
        \methodOne{1} applied to VQ-VAEs. The VQ-VAEs framework is extended by adding a second conditioned decoder. Instead of using a fully connected layer to merge the latent variable $z$ and the conditioning information $m$, the corresponding embedding table vectors $z_q(x)$ must be computed for the latent variable $z$ before merging it with the conditioning information $m$. As a result of adding the second conditioned decoder, the loss function requires the addition of the MSE reconstruction loss of the second decoder.
    }\label{SCVQVAE2DFigure}
\end{figure}

\section{\methodTwo{2}}

In the \methodTwo{2}, the idea is to employ a single decoder within the VAE architecture that is capable of reconstructing the input data under variable conditioning. Variable conditioning means that the conditioning information $m$ can be a variable amount of information or just an empty mask. This method differs from the \methodOne{1} in that it uses a single decoder for both tasks and dynamically adjusts its behavior based on the amount of information present in the variable $m$.

This method utilizes a single decoder that dynamically adjusts its behavior
based on the amount of information present in the variable $m$. When the
conditioning information is available, the decoder incorporates it into the
reconstruction process and takes advantage of it to get the best
reconstruction. Otherwise, it operates solely based on the latent variable $z$.

To achieve this, in this method, I will use different conditioning strategies, which will be described in the following subsection.

\subsection{Conditioning strategy}

Similar to the previous method, the conditioning information $m$ is acquired by
sampling pixels from the input image $x$. However, in this method, the process of sampling the conditioning information $m$ is different in that the conditioning information $m$ is sampled by selecting a variable number of pixels from the input image $x$. This is done to ensure that the decoder of the model learns to handle various cases where there is variable conditioning information.

To achieve this, I will implement a two-step sampling process:

\begin{enumerate}
    \item \textbf{Sampling the number of pixels:} In this step, the number of pixels to be sampled from the input image $x$ is sampled from a power law distribution. The power law distribution is scaled to the size of the input image $x$ and is used to sample the number of pixels to be sampled from the input image $x$.
    \item \textbf{Sampling the pixels:} In this step, the conditioning information $m$ is sampled from the input image $x$ by sampling the number of pixels sampled in the previous step from the input image: uniformly at random or from a Gaussian distribution. Both of these sampling types can be seen in Figure~\ref{SamplingFigure}.
\end{enumerate}

The sampling two-step process will be done every time the input image $x$ is passed through the model. This means that the conditioning information $m$ will be different for each time the image is passed through the model. This is done to ensure that the model learns to generalize and handle various cases.

During training, the number of pixels to be sampled from the input image $x$ varies every time the input image $x$ is passed through the model and is sampled from a power law distribution.
The power law distribution is chosen because it can have a finite range and scalability, which makes it suitable for sampling the number of pixels from the input image. The exponent of the power law distribution will be chosen to be high so that the decoder can learn to handle also the cases where the conditioning information is not available. The exponent will be a hyperparameter of the model.


\subsection{Application to Gaussian VAEs}

In Gaussian VAEs, applying the second method involves modifying the decoder to
be capable of reconstructing the input data under variable conditioning. The
modified decoder $p_\xi(x|z,m)$ is capable of reconstructing the input data
based on the latent variable $z$ and the conditioning information $m$ or just
the latent variable $z$ if the conditioning information is not available and is just an empty mask filled with zeros.

This is achieved by using a fully connected layer to merge the latent variable
$z$ and the conditioning information $m$ before passing it through the
transposed convolutional layers. The architecture of the Gaussian VAEs
framework with a single decoder capable of variable conditioning is illustrated
in Figure~\ref{SCVAE1DFigure}.

\begin{figure}[H]
    \centering
    \input{figures/scvae1d.tex}
    \caption[\methodTwo{2} applied to Gaussian VAEs.]%
    {
        \methodTwo\ applied to Gaussian VAEs. The Gaussian VAEs framework is modified by allowing the decoder to be conditioned on a variable amount of information and to dynamically adjust its behavior based on the amount of information present in the variable $m$. The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. The sampled $z$ is then used as input to the decoder as well as the extra conditioning information $m$. With this approach, there is no need for a second decoder, as the single decoder is capable of reconstructing the data based on the latent variable $z$ and the conditioning information $m$ or just the latent variable $z$.
    }\label{SCVAE1DFigure}
\end{figure}

\subsection{Application to VQ-VAEs}

In VQVAEs, the same as in Gaussian VAEs, the second method involves modifying the
decoder to be capable of receiving variable conditioning information. Unlike
Gaussian VAEs, the VQ-VAEs use Vector Quantization to discretize the continuous 
latent space, which is described in \autoref{background:vqvae}. Since the latent
variable $z$ is just a representation of the indexes of the embedding table vectors, the latent variable $z$ must be first mapped to the corresponding embedding table vectors resulting in $z_q(x)$ representation. Then the same process as in Gaussian VAEs is applied where the latent variable $z_q(x)$ and the conditioning information $m$ are merged using a fully connected layer before passing it through the transposed convolutional layers of the decoder. The architecture of the VQ-VAEs framework with a single decoder capable of
variable conditioning is illustrated in Figure~\ref{SCVQVAE1DFigure}.

\begin{figure}[H]
    \centering
    \input{figures/scvqvae1d.tex}
    \caption[\methodTwo{2} applied to VQ-VAEs.]%
    {
        \methodTwo\ applied to VQ-VAEs. The VQ-VAEs framework is modified by allowing the decoder to be conditioned on a variable amount of information and to dynamically adjust its behavior based on the amount of information present in the variable $m$. The input $x$ is passed through the encoder producing the output $z_e(x)$. The output $z_e(x)$ is then used as input to the autoregressive model to generate the latent variable $z$. The latent variable $z$ is then used as input to the decoder as well as the extra conditioning information $m$. With this approach, there is no need for a second decoder, as the single decoder is capable of reconstructing the data based on the latent variable $z$ and the conditioning information $m$ or just the latent variable $z$.
    }\label{SCVQVAE1DFigure}
\end{figure}

\begin{figure}
    \centering
    \input{figures/sampling.tex}
    \caption[Table of pixel sampling types for conditioning.]%
    {
        Table of pixel sampling types for conditioning. The table has 3 rows each representing a different sampling type. The first row represents the Exact Sampling type, the second row represents the uniform random sampling type and the third row represents the Gaussian sampling type where the pixels are more likely to be sampled from the center of the image. The first column represents the original image, the second column represents the mask and the third column represents the result of the sampling operation.
    }\label{SamplingFigure}
\end{figure}

\section{Experimental setup}

In this section, I will describe the experimental setup that was used to evaluate the proposed methods. The full code for experiments was implemented in Python using Pytorch library and is available on GitHub\footnote{\url{https://github.com/EdvardsZ/MastersThesis}}.

\subsection{Datasets}

The proposed methods were evaluated on the MNIST, CIFAR-10 and CelebA datasets. The MNIST dataset consists of 60,000 images of the size 28x28 pixels with 10 classes. The CIFAR-10 dataset consists of 60,000 colored images of the size 32x32 pixels with 10 classes.
The CelebA dataset consists of 202,599 images of celebrities with 10,177 identities and 40 binary attributes. In these experiments, the CelebA dataset images were resized to 64x64 pixels to reduce the computational complexity.
The datasets will be split into training, validation and test sets, where the training set will be used to train the models and the validation set will be used to evaluate.

\subsection{Network architecture and hyperparameters}

The encoder and the decoder of the models will be convolutional neural networks. The number of layers and the number of filters in each layer will be hyperparameters of the model. Both the encoder and the decoder will have a similar architecture of convolutional layers followed by batch normalization layers and LeakyReLU activation functions. However, the encoder and the decoder for the Gaussian VAEs will be more shallow compared to the VQ-VAEs, because the Gaussian VAEs suffer from the posterior collapse if the neural network is too deep. 

For Gaussian VAEs experiments were conducted in 2 configurations:

\begin{itemize}
    \item Config. 1: latent space dimension of 16, with hidden dimensions of 32, 64, 128, 256
    \item Config. 2: latent space dimension of 64, with hidden dimensions of 32, 64, 128, 256
\end{itemize}

For VQ-VAEs experiments were conducted in 3 configurations:

\begin{itemize}
    \item Config. 1: 128 embeddings and 16 the embedding dimension, hidden dimensions for both encoder and decoder were set to 32 and 64 and 0 residual layers.
    \item Config. 2: 128 embeddings and 32 the embedding dimension, hidden dimensions for both encoder and decoder were set to 128 and 256 and 2 residual layers.
    \item Config. 3: 256 embeddings and 64 the embedding dimension, hidden dimensions for both encoder and decoder were set to 128 and 256 and 4 residual layers.
\end{itemize}

\subsection{Training}

The models were trained using the AdamW optimizer with a learning rate of $10^{-3}$ and a batch size of 128 for 100 epochs. The best models were selected based on the cross-validation results. The models were trained on a single GPU.