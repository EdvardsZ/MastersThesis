\chapter{Methods}

This chapter will outline the methods how SCVAEs and non-conditioned VAEs are integrated using multitask learning in this thesis. Initially, describe the method of merging SCVAEs with non-conditioned VAEs by expanding the SCVAEs framework with a second decoder. This method will be applied to both Gaussian VAEs and VQ-VAEs. In the following section, I will describe another method of merging SCVAEs with non-conditioned VAEs by utilizing the same encoder and decoder for both models by employing novel training strategies. This method will also be applied to both Gaussian VAEs and VQ-VAEs.

\section{Method 1: Expanding the VAEs framework}

In the first method, we expand the VAEs framework with a second decoder $p_\xi(x|z,m)$, which is conditioned on an additional property $m$ of the input data. This approach involves utilizing two decoders within the VAEs framework: one for reconstructing the input data solely based on the latent variable $z$, and the other for reconstructing the input data based on both the latent variable $z$ and the additional conditioning information $m$.

\subsection{Conditioning information}

In this method, the conditioning information $m$ is acquired by sampling constant number of pixels from the input data. To sample the pixels from the input image $x$, we will use a pixel sampling operation, which will be described in the following section.

\subsubsection{Pixel Sampling operation}

To acquire the conditioning information $m$ from the input image $x$, we will use sampling operation. I will explore two sampling types for this method, which can be seen in Figure~\ref{SamplingFigure} and are described as follows: 

\begin{enumerate}
    \item \textbf{Exact same sampling:} In this sampling type, the conditioning information $m$ is sampled from the input image $x$ by sampling the exact same sparse pixels from the input image $x$. In this work, the pixels that will be sampled from the input image will be a sparse grid of pixels.
    \item \textbf{Uniform random sampling:} In this sampling type, the conditioning information $m$ is sampled from the input image $x$ by sampling exact number of pixels from the input image $x$ uniformly at random.
\end{enumerate}

\subsection{Application to Gaussian VAEs}

The application of this method to Gaussian VAEs is illustrated in Figure~\ref{SCVAE1DFigure}. 



\begin{figure}[H]
    \centering 
    \input{figures/scvae2d.tex}
    \caption[Method 1 applied to Gaussian VAEs.]%
    { 
        Method 1 applied to Gaussian VAEs. The Gaussian VAEs framework is extended by adding a second decoder $p_\xi(x|z,m)$, where $m$ is the conditioned information about the input data. Same as in 

        
        
        
        The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. Then the sampled $z$ is used as input to both decoders. The first decoder with parameters $\theta$ produces the output $\hat{x}$. The second decoder with parameters $\xi$ produces the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss.
        
        
        The sampled $z$ is then passed through the decoder with parameters $\theta$ producing the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss. 
    }\label{SCVAE1DFigure}
\end{figure}

\subsection{Application to VQ-VAEs}

\begin{figure}[H]
    \centering 
    \input{figures/scvqvae2d.tex}
    \caption[Architecture of SCVAE1D.]%
    { 
        Architecture of Gaussian VAEs. The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. The sampled $z$ is then passed through the decoder with parameters $\theta$ producing the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss. 
    }\label{SCVQVAE1DFigure}
\end{figure}

\section{Method 2}


The first decoder is used to reconstruct the input data, given some conditioned information $m$ about the input data and the latent variable. The second decoder is used to reconstruct the input data with just the latent variable $z$.

and the second decoder is used to reconstruct the input data, given no information about the input data. In this thesis we will explore how we can use sparse pixels from the input data as the information that 

The SCVAE1D method and its VQ-VAE counterpart SCVQVAE1D is 

\subsection{Conditioning information}



\begin{figure}
    \centering 
    \input{figures/sampling.tex}
    \caption[Table of Pixel sampling types for conditioning.]%
    { 
        Table of Pixel sampling types for conditioning. The table has 3 rows each representing a different sampling type. The first row represents the exact same sampling type, the second row represents the uniform random sampling type and the third row represents the Gaussian sampling type where the pixels are more likely to be sampled from the center of the image. The first column represents the original image, the second column represents the mask and the third column represents the result of the sampling operation. 
    }\label{SamplingFigure}
\end{figure}





