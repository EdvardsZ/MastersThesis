\chapter{Methods}

This chapter will outline the methods how SCVAEs and non-conditioned VAEs are integrated using multitask learning in this thesis. Initially, describe the method of merging SCVAEs with non-conditioned VAEs by expanding the SCVAEs framework with a second decoder. This method will be applied to both Gaussian VAEs and VQ-VAEs. In the following section, I will describe another method of merging SCVAEs with non-conditioned VAEs by utilizing the same encoder and decoder for both models by employing novel training strategies. This method will also be applied to both Gaussian VAEs and VQ-VAEs.

\section{Method 1: Expanding the VAEs framework}

In the first method, we expand the VAEs framework with a second decoder $p_\xi(x|z,m)$, where $m$ represents the conditioned information about the input data.  This approach involves utilizing two decoders within the VAEs framework: one for reconstructing the input data solely based on the latent variable $z$, and the other for reconstructing the input data based on both the latent variable $z$ and the conditioned information $m$. 

The first decoder uses only the latent variable $z$ to reconstruct the input data. However, the second decoder, which incorporates the conditioned information $m$, is adapted from the decoder architecture used in SCVAEs. This decoder architecture ensures effective utilization of conditioned information, leveraging the insights from SCVAEs~\cite{Gundersen_2021}.

\subsection{Conditioned information}

In this method, the conditioned information $m$ is acquired by sampling constant number of pixels from the input data. To sample the pixels from the input image $x$, we will use the sampling operation, which will be described in the following section.

\subsubsection{Sampling operation}

To acquire the conditioned information $m$ from the input image $x$, we will use sampling operation. I will explore two sampling types for this method:

\begin{enumerate}
    \item \textbf{Exact same sampling:} In this sampling type, the conditioned information $m$ is sampled from the input image $x$ by sampling the exact same sparse pixels from the input image $x$. In this work, the pixels that will be sampled from the input image will be a sparse grid of pixels.
    \item \textbf{Uniform random sampling:} In this sampling type, the conditioned information $m$ is sampled from the input image $x$ by sampling exact number of pixels from the input image $x$ uniformly at random.
\end{enumerate}



\subsection{Sampling operation}

I will use the sampling operation to acquire the conditioned information $m$ from the input image $x$. In this master's thesis we will explore 2 sampling types for this method. The first sampling type is to sample the conditioned information $m$ from the input image $x$ by sampling the exact same sparse pixels from the input image $x$. The second sampling type is to sample the conditioned information $m$ from the input image $x$ by sampling exact number of pixels from the input image $x$ uniformly at random. 

\begin{figure}[H]
    \centering 
    \input{figures/sampling.tex}
    \caption[Sampling types.]%
    { 
        Sampling types. The first sampling type is to sample the conditioned information $m$ from the input image $x$ by sampling the exact same sparse pixels from the input image $x$. The second sampling type is to sample the conditioned information $m$ from the input image $x$ by sampling exact number of pixels from the input image $x$ uniformly at random. 
    }\label{SamplingFigure}
\end{figure}

\subsection{Application to Gaussian VAEs}

\begin{figure}[H]
    \centering 
    \input{figures/scvae2d.tex}
    \caption[Method 1 applied to Gaussian VAEs.]%
    { 
        Method 1 applied to Gaussian VAEs. The Gaussian VAEs framework is extended by adding a second decoder $p_\xi(x|z,m)$, where $m$ is the conditioned information about the input data. Same as in 

        
        
        
        The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. Then the sampled $z$ is used as input to both decoders. The first decoder with parameters $\theta$ produces the output $\hat{x}$. The second decoder with parameters $\xi$ produces the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss.
        
        
        The sampled $z$ is then passed through the decoder with parameters $\theta$ producing the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss. 
    }\label{SCVAE1DFigure}
\end{figure}

\subsection{Application to VQ-VAEs}

\begin{figure}[H]
    \centering 
    \input{figures/scvqvae2d.tex}
    \caption[Architecture of SCVAE1D.]%
    { 
        Architecture of Gaussian VAEs. The input $x$ is passed through the encoder with parameters $\phi$ producing the mean $\mu$ and the standard deviation $\sigma$ of the Gaussian distribution. The random variable $\epsilon$ is sampled from a standard Gaussian distribution and is used to sample $ z = \mu + \sigma \odot \epsilon$. The sampled $z$ is then passed through the decoder with parameters $\theta$ producing the output $\hat{x}$. The loss function to be minimized is the sum of the MSE reconstruction loss and the KL divergence regularization loss. 
    }\label{SCVQVAE1DFigure}
\end{figure}

\section{Method 2}


The first decoder is used to reconstruct the input data, given some conditioned information $m$ about the input data and the latent variable. The second decoder is used to reconstruct the input data with just the latent variable $z$.

and the second decoder is used to reconstruct the input data, given no information about the input data. In this thesis we will explore how we can use sparse pixels from the input data as the information that 

The SCVAE1D method and its VQ-VAE counterpart SCVQVAE1D is 





