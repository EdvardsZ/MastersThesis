{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1; loss: 85.86835; acc: 0.81262:   0%|          | 3/1875 [00:03<28:32,  1.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1; loss: 11.91310; acc: 0.29050:   0%|          | 5/1875 [00:04<13:52,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1; loss: 11.91310; acc: 0.29050:   0%|          | 5/1875 [00:04<26:33,  1.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     train(i, loader, model, optimizer, device)\n\u001b[1;32m     60\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmnist_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(i\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mzfill(\u001b[39m3\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, loader, model, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m out, _ \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, img)\n\u001b[0;32m---> 23\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m _, pred \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_masters/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_masters/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pixelsnail import PixelSNAIL\n",
    "\n",
    "\n",
    "def train(epoch, loader, model, optimizer, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (img, label) in enumerate(loader):\n",
    "        model.zero_grad()\n",
    "\n",
    "        img = img.to(device)\n",
    "\n",
    "        out, _ = model(img)\n",
    "        loss = criterion(out, img)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = out.max(1)\n",
    "        correct = (pred == img).float()\n",
    "        accuracy = correct.sum() / img.numel()\n",
    "\n",
    "        loader.set_description(\n",
    "            (f'epoch: {epoch + 1}; loss: {loss.item():.5f}; ' f'acc: {accuracy:.5f}')\n",
    "        )\n",
    "\n",
    "\n",
    "class PixelTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input):\n",
    "        ar = np.array(input)\n",
    "\n",
    "        return torch.from_numpy(ar).long()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    device = 'cuda'\n",
    "    epoch = 10\n",
    "\n",
    "    dataset = datasets.MNIST('.', transform=PixelTransform(), download=True)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "    model = PixelSNAIL([28, 28], 256, 128, 5, 2, 4, 128)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for i in range(10):\n",
    "        train(i, loader, model, optimizer, device)\n",
    "        torch.save(model.state_dict(), f'mnist_{str(i + 1).zfill(3)}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmnist_010.pt\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     48\u001b[0m \u001b[39m# Generate an image\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m generated_image \u001b[39m=\u001b[39m generate_image(model, canvas)\n\u001b[1;32m     51\u001b[0m \u001b[39m# You can convert the generated_image tensor to a numpy array and visualize it as an image\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# generated_image_np = generated_image.numpy()\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m# ...\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39m# Optionally, you can save the generated image to a file\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# torch.save(generated_image, \"generated_image.pth\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mgenerate_image\u001b[0;34m(model, canvas)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(height):\n\u001b[1;32m     23\u001b[0m     \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(width):\n\u001b[0;32m---> 24\u001b[0m         pixel_value \u001b[39m=\u001b[39m sample_pixel(model, canvas, x, y)\n\u001b[1;32m     25\u001b[0m         canvas[:, :, x, y] \u001b[39m=\u001b[39m pixel_value\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m canvas\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36msample_pixel\u001b[0;34m(model, canvas, x, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Sample the pixel value for the current position\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mprint\u001b[39m(model(condition_input))\n\u001b[1;32m     13\u001b[0m     logits, _ \u001b[39m=\u001b[39m model(condition_input)\n\u001b[1;32m     14\u001b[0m     pixel_probabilities \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits[:, :, x, y], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_masters/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/MastersThesis/src_4/pixelsnail.py:400\u001b[0m, in \u001b[0;36mPixelSNAIL.forward\u001b[0;34m(self, input, condition, cache)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     cache \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 400\u001b[0m batch, height, width \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\n\u001b[1;32m    401\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m (\n\u001b[1;32m    402\u001b[0m     F\u001b[39m.\u001b[39mone_hot(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_class)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtype_as(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackground)\n\u001b[1;32m    403\u001b[0m )\n\u001b[1;32m    404\u001b[0m horizontal \u001b[39m=\u001b[39m shift_down(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhorizontal(\u001b[39minput\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pixelsnail import PixelSNAIL\n",
    "\n",
    "def sample_pixel(model, canvas, x, y):\n",
    "    # Condition the model on the already sampled pixels (left and above)\n",
    "    condition_input = canvas[:, :, :x, :y]\n",
    "    \n",
    "    # Sample the pixel value for the current position\n",
    "    with torch.no_grad():\n",
    "        print(model(condition_input))\n",
    "        logits, _ = model(condition_input)\n",
    "        pixel_probabilities = F.softmax(logits[:, :, x, y], dim=-1)\n",
    "        pixel_value = torch.multinomial(pixel_probabilities, 1).squeeze()\n",
    "    \n",
    "    return pixel_value\n",
    "\n",
    "def generate_image(model, canvas):\n",
    "    batch_size, channel, height, width = canvas.shape\n",
    "    \n",
    "    for x in range(height):\n",
    "        for y in range(width):\n",
    "            pixel_value = sample_pixel(model, canvas, x, y)\n",
    "            canvas[:, :, x, y] = pixel_value\n",
    "    \n",
    "    return canvas\n",
    "\n",
    "# Define the shape of the image and number of classes\n",
    "image_shape = (32,1, 28, 28)  # (batch_size, channels, height, width)\n",
    "num_classes = 256  # Assuming you have 256 possible pixel values\n",
    "\n",
    "# Initialize an empty canvas\n",
    "canvas = torch.zeros(image_shape)\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "# Load your trained PixelSNAIL model\n",
    "# trained_model = ...\n",
    "\n",
    "# load trained model from file\n",
    "model = PixelSNAIL([28, 28], 256, 128, 5, 2, 4, 128)\n",
    "model.load_state_dict(torch.load('mnist_010.pt'))\n",
    "\n",
    "\n",
    "# Generate an image\n",
    "generated_image = generate_image(model, canvas)\n",
    "\n",
    "# You can convert the generated_image tensor to a numpy array and visualize it as an image\n",
    "# generated_image_np = generated_image.numpy()\n",
    "# ...\n",
    "\n",
    "# Optionally, you can save the generated image to a file\n",
    "# torch.save(generated_image, \"generated_image.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
