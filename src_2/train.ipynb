{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "mnist_dataloader = torch.utils.data.DataLoader(dataset=mnist_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim: int, h_dim1: int, h_dim2: int, z_dim: int):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "    def loss(self, batch, outputs):\n",
    "        x, y = batch\n",
    "        x_recon, mean, log_var  = outputs\n",
    "\n",
    "        BCE = F.binary_cross_entropy(x_recon, x.view(-1, 784), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        \n",
    "        loss = BCE + KLD\n",
    "\n",
    "        return { 'loss': loss, 'BCE_loss': BCE, 'KLD_loss': KLD}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  tensor([1, 1])\n",
      "Epoch: 1, Loss: 16115.380859375, BCE Loss: 15577.8779296875, KLD Loss: 537.503662109375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 2, Loss: 15348.4775390625, BCE Loss: 14779.9765625, KLD Loss: 568.5037841796875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 3, Loss: 15107.86328125, BCE Loss: 14508.21875, KLD Loss: 599.6446533203125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 4, Loss: 14884.916015625, BCE Loss: 14296.375, KLD Loss: 588.5421752929688\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 5, Loss: 14752.2138671875, BCE Loss: 14120.1953125, KLD Loss: 632.0170288085938\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 6, Loss: 14650.7724609375, BCE Loss: 14012.6337890625, KLD Loss: 638.1389770507812\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 7, Loss: 14595.6689453125, BCE Loss: 13957.1474609375, KLD Loss: 638.5204467773438\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 8, Loss: 14520.6953125, BCE Loss: 13889.115234375, KLD Loss: 631.5807495117188\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 9, Loss: 14497.978515625, BCE Loss: 13855.8271484375, KLD Loss: 642.1504516601562\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 10, Loss: 14384.771484375, BCE Loss: 13721.130859375, KLD Loss: 663.6408081054688\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 11, Loss: 14384.27734375, BCE Loss: 13728.8974609375, KLD Loss: 655.3796997070312\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 12, Loss: 14263.865234375, BCE Loss: 13602.3447265625, KLD Loss: 661.521484375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 13, Loss: 14296.04296875, BCE Loss: 13639.365234375, KLD Loss: 656.677490234375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 14, Loss: 14179.2470703125, BCE Loss: 13504.23046875, KLD Loss: 675.0178833007812\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 15, Loss: 14222.638671875, BCE Loss: 13524.9921875, KLD Loss: 697.64501953125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 16, Loss: 14162.763671875, BCE Loss: 13477.4296875, KLD Loss: 685.3333740234375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 17, Loss: 14117.3154296875, BCE Loss: 13431.1552734375, KLD Loss: 686.16015625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 18, Loss: 14143.8779296875, BCE Loss: 13470.6064453125, KLD Loss: 673.2715454101562\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 19, Loss: 14119.6953125, BCE Loss: 13452.0234375, KLD Loss: 667.6710815429688\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 20, Loss: 14102.5927734375, BCE Loss: 13409.7177734375, KLD Loss: 692.8736572265625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 21, Loss: 14042.7646484375, BCE Loss: 13356.93359375, KLD Loss: 685.8314208984375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 22, Loss: 14068.8720703125, BCE Loss: 13380.486328125, KLD Loss: 688.3847045898438\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 23, Loss: 14030.18359375, BCE Loss: 13335.21484375, KLD Loss: 694.9682006835938\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 24, Loss: 14003.3974609375, BCE Loss: 13314.958984375, KLD Loss: 688.439208984375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 25, Loss: 14013.9833984375, BCE Loss: 13323.212890625, KLD Loss: 690.7720947265625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 26, Loss: 14001.875, BCE Loss: 13295.2275390625, KLD Loss: 706.6484985351562\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 27, Loss: 14039.044921875, BCE Loss: 13333.798828125, KLD Loss: 705.24658203125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 28, Loss: 14024.3076171875, BCE Loss: 13321.9287109375, KLD Loss: 702.3778686523438\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 29, Loss: 13980.58984375, BCE Loss: 13292.9326171875, KLD Loss: 687.6574096679688\n"
     ]
    }
   ],
   "source": [
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for current_epoch in range(1, 30):\n",
    "    for x, y in mnist_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        bce_losses = []\n",
    "        kld_losses = []\n",
    "\n",
    "        for x, y in val_dataloader:\n",
    "            x_recon, mean, log_var = model(x, y)\n",
    "            loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "            losses.append(loss['loss'])\n",
    "            bce_losses.append(loss['BCE_loss'])\n",
    "            kld_losses.append(loss['KLD_loss'])\n",
    "        print(\"Weights: \", torch.tensor([1,1]))\n",
    "        print(\"Epoch: {}, Loss: {}, BCE Loss: {}, KLD Loss: {}\".format(current_epoch, torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(bce_losses)), torch.mean(torch.tensor(kld_losses))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train example with softadapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  tensor([0.9193, 0.0807], dtype=torch.float64)\n",
      "Epoch: 1, Loss: 16606.33203125, BCE Loss: 15405.9677734375, KLD Loss: 1200.3646240234375\n",
      "Weights:  tensor([0.9351, 0.0649], dtype=torch.float64)\n",
      "Epoch: 2, Loss: 15907.150390625, BCE Loss: 14603.6572265625, KLD Loss: 1303.4937744140625\n",
      "Weights:  tensor([0.9129, 0.0871], dtype=torch.float64)\n",
      "Epoch: 3, Loss: 15421.130859375, BCE Loss: 14231.5478515625, KLD Loss: 1189.5838623046875\n",
      "Weights:  tensor([0.8923, 0.1077], dtype=torch.float64)\n",
      "Epoch: 4, Loss: 15127.05859375, BCE Loss: 14032.9853515625, KLD Loss: 1094.0736083984375\n",
      "Weights:  tensor([0.9140, 0.0860], dtype=torch.float64)\n",
      "Epoch: 5, Loss: 15001.2802734375, BCE Loss: 13837.037109375, KLD Loss: 1164.2423095703125\n",
      "Weights:  tensor([0.9590, 0.0410], dtype=torch.float64)\n",
      "Epoch: 6, Loss: 15024.3623046875, BCE Loss: 13809.22265625, KLD Loss: 1215.1402587890625\n",
      "Weights:  tensor([0.9121, 0.0879], dtype=torch.float64)\n",
      "Epoch: 7, Loss: 14791.1787109375, BCE Loss: 13656.8037109375, KLD Loss: 1134.37451171875\n",
      "Weights:  tensor([0.9531, 0.0469], dtype=torch.float64)\n",
      "Epoch: 8, Loss: 14798.09765625, BCE Loss: 13581.5888671875, KLD Loss: 1216.50830078125\n",
      "Weights:  tensor([0.9530, 0.0470], dtype=torch.float64)\n",
      "Epoch: 9, Loss: 14718.705078125, BCE Loss: 13507.8046875, KLD Loss: 1210.9012451171875\n",
      "Weights:  tensor([0.9418, 0.0582], dtype=torch.float64)\n",
      "Epoch: 10, Loss: 14591.4951171875, BCE Loss: 13455.8466796875, KLD Loss: 1135.64892578125\n",
      "Weights:  tensor([0.9137, 0.0863], dtype=torch.float64)\n",
      "Epoch: 11, Loss: 14515.5185546875, BCE Loss: 13425.6220703125, KLD Loss: 1089.89599609375\n",
      "Weights:  tensor([0.9515, 0.0485], dtype=torch.float64)\n",
      "Epoch: 12, Loss: 14526.01953125, BCE Loss: 13379.91015625, KLD Loss: 1146.1092529296875\n",
      "Weights:  tensor([0.9083, 0.0917], dtype=torch.float64)\n",
      "Epoch: 13, Loss: 14426.5849609375, BCE Loss: 13352.3134765625, KLD Loss: 1074.270751953125\n",
      "Weights:  tensor([0.9351, 0.0649], dtype=torch.float64)\n",
      "Epoch: 14, Loss: 14429.06640625, BCE Loss: 13315.9853515625, KLD Loss: 1113.0802001953125\n",
      "Weights:  tensor([0.9420, 0.0580], dtype=torch.float64)\n",
      "Epoch: 15, Loss: 14370.72265625, BCE Loss: 13236.1689453125, KLD Loss: 1134.554931640625\n",
      "Weights:  tensor([0.8322, 0.1678], dtype=torch.float64)\n",
      "Epoch: 16, Loss: 14259.912109375, BCE Loss: 13230.009765625, KLD Loss: 1029.90478515625\n",
      "Weights:  tensor([0.9293, 0.0707], dtype=torch.float64)\n",
      "Epoch: 17, Loss: 14329.1103515625, BCE Loss: 13225.677734375, KLD Loss: 1103.4315185546875\n",
      "Weights:  tensor([0.9061, 0.0939], dtype=torch.float64)\n",
      "Epoch: 18, Loss: 14267.4375, BCE Loss: 13203.6533203125, KLD Loss: 1063.783935546875\n",
      "Weights:  tensor([0.9180, 0.0820], dtype=torch.float64)\n",
      "Epoch: 19, Loss: 14318.7626953125, BCE Loss: 13262.2939453125, KLD Loss: 1056.46826171875\n",
      "Weights:  tensor([0.9206, 0.0794], dtype=torch.float64)\n",
      "Epoch: 20, Loss: 14278.1376953125, BCE Loss: 13171.7626953125, KLD Loss: 1106.3763427734375\n",
      "Weights:  tensor([0.9394, 0.0606], dtype=torch.float64)\n",
      "Epoch: 21, Loss: 14236.2509765625, BCE Loss: 13136.3701171875, KLD Loss: 1099.8809814453125\n",
      "Weights:  tensor([0.9501, 0.0499], dtype=torch.float64)\n",
      "Epoch: 22, Loss: 14258.90234375, BCE Loss: 13138.9296875, KLD Loss: 1119.97314453125\n",
      "Weights:  tensor([0.9530, 0.0470], dtype=torch.float64)\n",
      "Epoch: 23, Loss: 14248.724609375, BCE Loss: 13130.611328125, KLD Loss: 1118.1131591796875\n",
      "Weights:  tensor([0.8794, 0.1206], dtype=torch.float64)\n",
      "Epoch: 24, Loss: 14204.1728515625, BCE Loss: 13128.6748046875, KLD Loss: 1075.4984130859375\n",
      "Weights:  tensor([0.9582, 0.0418], dtype=torch.float64)\n",
      "Epoch: 25, Loss: 14351.7939453125, BCE Loss: 13188.1025390625, KLD Loss: 1163.69189453125\n",
      "Weights:  tensor([0.9098, 0.0902], dtype=torch.float64)\n",
      "Epoch: 26, Loss: 14126.3076171875, BCE Loss: 13062.8125, KLD Loss: 1063.4964599609375\n",
      "Weights:  tensor([0.8358, 0.1642], dtype=torch.float64)\n",
      "Epoch: 27, Loss: 14070.525390625, BCE Loss: 13061.2333984375, KLD Loss: 1009.2901611328125\n",
      "Weights:  tensor([0.9352, 0.0648], dtype=torch.float64)\n",
      "Epoch: 28, Loss: 14161.294921875, BCE Loss: 13024.6923828125, KLD Loss: 1136.6019287109375\n",
      "Weights:  tensor([0.8784, 0.1216], dtype=torch.float64)\n",
      "Epoch: 29, Loss: 14113.9921875, BCE Loss: 13086.224609375, KLD Loss: 1027.767578125\n"
     ]
    }
   ],
   "source": [
    "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt\n",
    "\n",
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Change 1: Create a SoftAdapt object (with your desired variant)\n",
    "softadapt_object = LossWeightedSoftAdapt(beta=0.001)\n",
    "\n",
    "# Change 2: Define how often SoftAdapt calculate weights for the loss components\n",
    "epochs_to_make_updates = 5\n",
    "\n",
    "values_of_component_1 = []\n",
    "values_of_component_2 = []\n",
    "# Initializing adaptive weights to all ones.\n",
    "adapt_weights = torch.tensor([1,1])\n",
    "\n",
    "limit = 101\n",
    "\n",
    "count = 0\n",
    "\n",
    "for current_epoch in range(1, 30):\n",
    "    for x, y in mnist_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        count += 1\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "\n",
    "        bce_loss = loss['BCE_loss']\n",
    "        kld = loss['KLD_loss']\n",
    "\n",
    "        values_of_component_1.append(bce_loss)\n",
    "        values_of_component_2.append(kld)\n",
    "\n",
    "        if (current_epoch % epochs_to_make_updates == 0 and current_epoch > 1 and count >= limit) or count >= limit:\n",
    "            # Change 3: Update weights of components\n",
    "            count = 0\n",
    "            # print(\"Adaptive weights: \", adapt_weights)\n",
    "            # print(\"epoch\")\n",
    "            # print(current_epoch)\n",
    "            first = torch.tensor(values_of_component_1, dtype=torch.float64)\n",
    "            second = torch.tensor(values_of_component_2, dtype=torch.float64)\n",
    "            # print(first)\n",
    "            # print(second)\n",
    "            # print(first.dtype)\n",
    "            # print(second.dtype)\n",
    "            # print(first.shape)\n",
    "            # print(second.shape)\n",
    "            adapt_weights = softadapt_object.get_component_weights(first, second,verbose=False)\n",
    "            #print(\"WORKS\")\n",
    "                                           \n",
    "        \n",
    "            # Resetting the lists to start fresh (this part is optional)\n",
    "            values_of_component_1 = []\n",
    "            values_of_component_2 = []\n",
    "\n",
    "        loss = adapt_weights[0] * bce_loss + adapt_weights[1] * kld\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        bce_losses = []\n",
    "        kld_losses = []\n",
    "\n",
    "        for x, y in val_dataloader:\n",
    "            x_recon, mean, log_var = model(x, y)\n",
    "            loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "            losses.append(loss['loss'])\n",
    "            bce_losses.append(loss['BCE_loss'])\n",
    "            kld_losses.append(loss['KLD_loss'])\n",
    "        print(\"Weights: \", adapt_weights)\n",
    "        print(\"Epoch: {}, Loss: {}, BCE Loss: {}, KLD Loss: {}\".format(current_epoch, torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(bce_losses)), torch.mean(torch.tensor(kld_losses))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
