{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "mnist_dataloader = torch.utils.data.DataLoader(dataset=mnist_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim: int, h_dim1: int, h_dim2: int, z_dim: int):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "    def loss(self, batch, outputs):\n",
    "        x, y = batch\n",
    "        x_recon, mean, log_var  = outputs\n",
    "\n",
    "        BCE = F.binary_cross_entropy(x_recon, x.view(-1, 784), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        \n",
    "        loss = BCE + KLD\n",
    "\n",
    "        return { 'loss': loss, 'BCE_loss': BCE, 'KLD_loss': KLD}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train example with softadapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 18579.5293, BCE Loss: 18315.3633, KLD Loss: 264.1652\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [2/30], Loss: 15295.5254, BCE Loss: 16298.4072, KLD Loss: 988.6555\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [3/30], Loss: 15481.9363, BCE Loss: 16080.1582, KLD Loss: 1181.3688\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [4/30], Loss: 14472.2100, BCE Loss: 15376.2148, KLD Loss: 1160.6482\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [5/30], Loss: 13440.5237, BCE Loss: 15070.9453, KLD Loss: 1161.4460\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [6/30], Loss: 13651.0323, BCE Loss: 15009.4131, KLD Loss: 1322.2211\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [7/30], Loss: 14885.5249, BCE Loss: 15496.2891, KLD Loss: 1287.3579\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [8/30], Loss: 13678.7245, BCE Loss: 14722.3594, KLD Loss: 1338.7231\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [9/30], Loss: 12315.6289, BCE Loss: 14452.1777, KLD Loss: 896.4022\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [10/30], Loss: 14161.9906, BCE Loss: 14501.7090, KLD Loss: 1372.0688\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [11/30], Loss: 12109.3888, BCE Loss: 14546.5098, KLD Loss: 1014.1212\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [12/30], Loss: 13488.7750, BCE Loss: 14594.7607, KLD Loss: 1179.7660\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [13/30], Loss: 12453.4359, BCE Loss: 14067.5410, KLD Loss: 1177.1241\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [14/30], Loss: 14380.2362, BCE Loss: 15632.2588, KLD Loss: 1134.4796\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [15/30], Loss: 13749.0725, BCE Loss: 14697.8809, KLD Loss: 1147.2549\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [16/30], Loss: 13120.7839, BCE Loss: 13678.5859, KLD Loss: 1359.1185\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [17/30], Loss: 11938.8099, BCE Loss: 13581.7949, KLD Loss: 1114.3168\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [18/30], Loss: 13478.3917, BCE Loss: 14299.1143, KLD Loss: 1197.2703\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [19/30], Loss: 13127.2506, BCE Loss: 14023.2334, KLD Loss: 1224.9919\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [20/30], Loss: 13582.8777, BCE Loss: 14171.8506, KLD Loss: 1244.5824\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [21/30], Loss: 12071.6222, BCE Loss: 13337.1895, KLD Loss: 1154.9393\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [22/30], Loss: 13205.8312, BCE Loss: 13773.2510, KLD Loss: 1269.3882\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [23/30], Loss: 12680.1886, BCE Loss: 13423.1572, KLD Loss: 1157.4927\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [24/30], Loss: 12097.1402, BCE Loss: 13534.2285, KLD Loss: 1226.6245\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [25/30], Loss: 12594.4052, BCE Loss: 13919.3926, KLD Loss: 1046.3153\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [26/30], Loss: 12695.3384, BCE Loss: 14161.3633, KLD Loss: 1030.5242\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [27/30], Loss: 12264.3077, BCE Loss: 12922.9326, KLD Loss: 1311.4695\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [28/30], Loss: 11955.1404, BCE Loss: 13332.8525, KLD Loss: 1188.2572\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "==> Interpreting finite difference order as 100 sinceno explicit order was specified.\n",
      "Epoch [29/30], Loss: 12871.4320, BCE Loss: 13607.3750, KLD Loss: 1140.8484\n"
     ]
    }
   ],
   "source": [
    "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt\n",
    "\n",
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Change 1: Create a SoftAdapt object (with your desired variant)\n",
    "softadapt_object = LossWeightedSoftAdapt(beta=0.001)\n",
    "\n",
    "# Change 2: Define how often SoftAdapt calculate weights for the loss components\n",
    "epochs_to_make_updates = 5\n",
    "\n",
    "values_of_component_1 = []\n",
    "values_of_component_2 = []\n",
    "# Initializing adaptive weights to all ones.\n",
    "adapt_weights = torch.tensor([1,1])\n",
    "\n",
    "limit = 101\n",
    "\n",
    "count = 0\n",
    "\n",
    "for current_epoch in range(1, 30):\n",
    "    for x, y in mnist_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        count += 1\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "\n",
    "        bce_loss = loss['BCE_loss']\n",
    "        kld = loss['KLD_loss']\n",
    "\n",
    "        values_of_component_1.append(bce_loss)\n",
    "        values_of_component_2.append(kld)\n",
    "\n",
    "        if (current_epoch % epochs_to_make_updates == 0 and current_epoch > 1 and count >= limit) or count >= limit:\n",
    "            # Change 3: Update weights of components\n",
    "            count = 0\n",
    "            # print(\"Adaptive weights: \", adapt_weights)\n",
    "            # print(\"epoch\")\n",
    "            # print(current_epoch)\n",
    "            first = torch.tensor(values_of_component_1, dtype=torch.float64)\n",
    "            second = torch.tensor(values_of_component_2, dtype=torch.float64)\n",
    "            # print(first)\n",
    "            # print(second)\n",
    "            # print(first.dtype)\n",
    "            # print(second.dtype)\n",
    "            # print(first.shape)\n",
    "            # print(second.shape)\n",
    "            adapt_weights = softadapt_object.get_component_weights(first, second,verbose=True)\n",
    "            #print(\"WORKS\")\n",
    "                                           \n",
    "        \n",
    "            # Resetting the lists to start fresh (this part is optional)\n",
    "            values_of_component_1 = []\n",
    "            values_of_component_2 = []\n",
    "\n",
    "        loss = adapt_weights[0] * bce_loss + adapt_weights[1] * kld\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{current_epoch}/{30}], Loss: {loss.item():.4f}, BCE Loss: {bce_loss.item():.4f}, KLD Loss: {kld.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
