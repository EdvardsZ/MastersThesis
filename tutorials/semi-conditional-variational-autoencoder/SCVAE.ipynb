{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ConditionalMNIST import load_mnist, get_observation_pixels\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader, test_loader, val_loader = load_mnist(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = next(iter(test_loader))[0][0][0].numpy()\n",
    "plt.imshow(example)\n",
    "# print on top of the image the observation pixels\n",
    "obs_x, obs_y = get_observation_pixels()\n",
    "for i in range(len(obs_x)):\n",
    "    plt.text(obs_x[i], obs_y[i], 'X', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from SCVAE import ConditionalVAE\n",
    "\n",
    "class SCVAE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ConditionalVAE = ConditionalVAE()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ConditionalVAE(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, x_cond, y = batch\n",
    "        output, z_mean, z_log_var, z = self.ConditionalVAE(x, x_cond)\n",
    "        recon_loss, kl_loss, loss = self.ConditionalVAE.loss(x, output, z_mean, z_log_var)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_recon_loss', recon_loss)\n",
    "        self.log('train_kl_loss', kl_loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, x_cond, y = batch\n",
    "        output, z_mean, z_log_var, z = self.ConditionalVAE(x, x_cond)\n",
    "        recon_loss, kl_loss, loss = self.ConditionalVAE.loss(x, output, z_mean, z_log_var)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_recon_loss', recon_loss)\n",
    "        self.log('val_kl_loss', kl_loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, x_cond, y = batch\n",
    "        output, z_mean, z_log_var, z = self.ConditionalVAE(x, x_cond)\n",
    "        recon_loss, kl_loss, loss = self.ConditionalVAE.loss(x, output, z_mean, z_log_var)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_recon_loss', recon_loss)\n",
    "        self.log('test_kl_loss', kl_loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:  (1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ ConditionalVAE │ ConditionalVAE │ 10.7 M │\n",
       "└───┴────────────────┴────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ ConditionalVAE │ ConditionalVAE │ 10.7 M │\n",
       "└───┴────────────────┴────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 10.7 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 10.7 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 42                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 10.7 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 10.7 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 42                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197c3022ed64488abaec268387c852d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [11,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [25,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [26,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [27,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [29,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [37,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [20,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [22,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [23,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [47,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:213\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 213\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:202\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[0;32m--> 202\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[39m# would be skipped otherwise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    251\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:370\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    369\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39musing_native_amp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprecision_plugin, MixedPrecisionPlugin)\n\u001b[0;32m--> 370\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    371\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    373\u001b[0m     batch_idx,\n\u001b[1;32m    374\u001b[0m     optimizer,\n\u001b[1;32m    375\u001b[0m     opt_idx,\n\u001b[1;32m    376\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    377\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    378\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1356\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1356\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1358\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1742\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[39mOverride this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m \u001b[39meach optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    235\u001b[0m     optimizer, model\u001b[39m=\u001b[39;49mmodel, optimizer_idx\u001b[39m=\u001b[39;49mopt_idx, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    236\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:119\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/optim/adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 183\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    185\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:105\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39mThe closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mconsistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:149\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:144\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward_fn(step_output\u001b[39m.\u001b[39;49mclosure_loss)\n\u001b[1;32m    146\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:305\u001b[0m, in \u001b[0;36mOptimizerLoop._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_fn\u001b[39m(loss: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m\"\u001b[39;49m, loss, optimizer, opt_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1496\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:207\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpre_backward(closure_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module)\n\u001b[0;32m--> 207\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49mbackward(closure_loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, optimizer, optimizer_idx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    209\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpost_backward(closure_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:67\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[0;34m(self, tensor, model, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    \\**kwargs: Keyword arguments for the same purpose as ``*args``.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m model\u001b[39m.\u001b[39;49mbackward(tensor, optimizer, optimizer_idx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1486\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m scvae \u001b[39m=\u001b[39m SCVAE()\n\u001b[1;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m, devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, callbacks\u001b[39m=\u001b[39m[RichProgressBar()])\n\u001b[0;32m----> 4\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(scvae, train_loader, val_loader)\n\u001b[1;32m      5\u001b[0m \u001b[39m#save\u001b[39;00m\n\u001b[1;32m      6\u001b[0m trainer\u001b[39m.\u001b[39msave_checkpoint(\u001b[39m'\u001b[39m\u001b[39mscvae.ckpt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:63\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m logger \u001b[39min\u001b[39;00m trainer\u001b[39m.\u001b[39mloggers:\n\u001b[1;32m     62\u001b[0m     logger\u001b[39m.\u001b[39mfinalize(\u001b[39m\"\u001b[39m\u001b[39mfailed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m trainer\u001b[39m.\u001b[39;49m_teardown()\n\u001b[1;32m     64\u001b[0m \u001b[39m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[1;32m     65\u001b[0m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1175\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_teardown\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[39m    Callback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mteardown()\n\u001b[1;32m   1176\u001b[0m     loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_active_loop\n\u001b[1;32m   1177\u001b[0m     \u001b[39m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:496\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: moving model to CPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 496\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m    497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mteardown()\n\u001b[1;32m    498\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py:78\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__update_properties(device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcpu()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/nn/modules/module.py:798\u001b[0m, in \u001b[0;36mModule.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcpu\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    790\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \n\u001b[1;32m    792\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcpu())\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_2/lib/python3.10/site-packages/torch/nn/modules/module.py:798\u001b[0m, in \u001b[0;36mModule.cpu.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcpu\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    790\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \n\u001b[1;32m    792\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcpu())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "scvae = SCVAE()\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=5, enable_progress_bar=True, callbacks=[RichProgressBar()])\n",
    "trainer.fit(scvae, train_loader, val_loader)\n",
    "#save\n",
    "trainer.save_checkpoint('scvae.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8660a498e4e826e1a81fc53ea2c15c710c53afb43a5f9617f6e01fc0625c90f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
