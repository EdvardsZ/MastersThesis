{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "mnist_dataloader = torch.utils.data.DataLoader(dataset=mnist_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim: int, h_dim1: int, h_dim2: int, z_dim: int):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "    def loss(self, batch, outputs):\n",
    "        x, y = batch\n",
    "        x_recon, mean, log_var  = outputs\n",
    "\n",
    "        BCE = recon_loss(x_recon, x.view(-1, 784))\n",
    "        KLD = kl_loss(mean, log_var)\n",
    "        \n",
    "        loss = BCE + KLD\n",
    "\n",
    "        return { 'loss': loss, 'BCE_loss': BCE, 'KLD_loss': KLD}\n",
    "    \n",
    "def kl_loss(z_mean, z_log_var):\n",
    "        return -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    \n",
    "def recon_loss(inputs, outputs):\n",
    "    return F.mse_loss(inputs, outputs, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  tensor([1, 1])\n",
      "Epoch: 1, Loss: 4163.302734375, BCE Loss: 3809.1845703125, KLD Loss: 354.11761474609375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 2, Loss: 3935.875, BCE Loss: 3529.89306640625, KLD Loss: 405.9815673828125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 3, Loss: 3798.40869140625, BCE Loss: 3353.83349609375, KLD Loss: 444.57537841796875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 4, Loss: 3714.271484375, BCE Loss: 3254.0146484375, KLD Loss: 460.25726318359375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 5, Loss: 3666.865966796875, BCE Loss: 3198.347900390625, KLD Loss: 468.5180358886719\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 6, Loss: 3642.686767578125, BCE Loss: 3136.531982421875, KLD Loss: 506.1553955078125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 7, Loss: 3600.828125, BCE Loss: 3099.433837890625, KLD Loss: 501.39385986328125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 8, Loss: 3579.992431640625, BCE Loss: 3091.715576171875, KLD Loss: 488.2769775390625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 9, Loss: 3547.362548828125, BCE Loss: 3048.224609375, KLD Loss: 499.1383972167969\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 10, Loss: 3533.648193359375, BCE Loss: 3028.97998046875, KLD Loss: 504.6676940917969\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 11, Loss: 3529.905029296875, BCE Loss: 3004.240966796875, KLD Loss: 525.6644897460938\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 12, Loss: 3504.248779296875, BCE Loss: 2979.9140625, KLD Loss: 524.33447265625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 13, Loss: 3522.5126953125, BCE Loss: 3015.14501953125, KLD Loss: 507.36749267578125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 14, Loss: 3482.19677734375, BCE Loss: 2949.850830078125, KLD Loss: 532.34619140625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 15, Loss: 3470.646484375, BCE Loss: 2943.250244140625, KLD Loss: 527.3965454101562\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 16, Loss: 3477.27197265625, BCE Loss: 2960.258056640625, KLD Loss: 517.0135498046875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 17, Loss: 3479.06689453125, BCE Loss: 2959.00634765625, KLD Loss: 520.060791015625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 18, Loss: 3453.202880859375, BCE Loss: 2924.22021484375, KLD Loss: 528.982666015625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 19, Loss: 3464.49365234375, BCE Loss: 2942.086181640625, KLD Loss: 522.4075927734375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 20, Loss: 3452.349609375, BCE Loss: 2920.11279296875, KLD Loss: 532.2362060546875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 21, Loss: 3441.99365234375, BCE Loss: 2903.3955078125, KLD Loss: 538.5984497070312\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 22, Loss: 3455.057861328125, BCE Loss: 2928.10791015625, KLD Loss: 526.9500122070312\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 23, Loss: 3436.600830078125, BCE Loss: 2907.52685546875, KLD Loss: 529.0741577148438\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 24, Loss: 3435.95068359375, BCE Loss: 2909.326171875, KLD Loss: 526.6243286132812\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 25, Loss: 3418.5654296875, BCE Loss: 2869.67431640625, KLD Loss: 548.8912963867188\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 26, Loss: 3424.207275390625, BCE Loss: 2879.449951171875, KLD Loss: 544.75732421875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 27, Loss: 3421.590576171875, BCE Loss: 2880.021240234375, KLD Loss: 541.5694580078125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 28, Loss: 3415.89404296875, BCE Loss: 2865.777099609375, KLD Loss: 550.116943359375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 29, Loss: 3427.651611328125, BCE Loss: 2881.3056640625, KLD Loss: 546.34619140625\n"
     ]
    }
   ],
   "source": [
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for current_epoch in range(1, 30):\n",
    "    for x, y in mnist_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        bce_losses = []\n",
    "        kld_losses = []\n",
    "\n",
    "        for x, y in val_dataloader:\n",
    "            x_recon, mean, log_var = model(x, y)\n",
    "            loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "            losses.append(loss['loss'])\n",
    "            bce_losses.append(loss['BCE_loss'])\n",
    "            kld_losses.append(loss['KLD_loss'])\n",
    "        print(\"Weights: \", torch.tensor([1,1]))\n",
    "        print(\"Epoch: {}, Loss: {}, BCE Loss: {}, KLD Loss: {}\".format(current_epoch, torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(bce_losses)), torch.mean(torch.tensor(kld_losses))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train example with softadapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  tensor([0.9193, 0.0807], dtype=torch.float64)\n",
      "Epoch: 1, Loss: 16606.33203125, BCE Loss: 15405.9677734375, KLD Loss: 1200.3646240234375\n",
      "Weights:  tensor([0.9351, 0.0649], dtype=torch.float64)\n",
      "Epoch: 2, Loss: 15907.150390625, BCE Loss: 14603.6572265625, KLD Loss: 1303.4937744140625\n",
      "Weights:  tensor([0.9129, 0.0871], dtype=torch.float64)\n",
      "Epoch: 3, Loss: 15421.130859375, BCE Loss: 14231.5478515625, KLD Loss: 1189.5838623046875\n",
      "Weights:  tensor([0.8923, 0.1077], dtype=torch.float64)\n",
      "Epoch: 4, Loss: 15127.05859375, BCE Loss: 14032.9853515625, KLD Loss: 1094.0736083984375\n",
      "Weights:  tensor([0.9140, 0.0860], dtype=torch.float64)\n",
      "Epoch: 5, Loss: 15001.2802734375, BCE Loss: 13837.037109375, KLD Loss: 1164.2423095703125\n",
      "Weights:  tensor([0.9590, 0.0410], dtype=torch.float64)\n",
      "Epoch: 6, Loss: 15024.3623046875, BCE Loss: 13809.22265625, KLD Loss: 1215.1402587890625\n",
      "Weights:  tensor([0.9121, 0.0879], dtype=torch.float64)\n",
      "Epoch: 7, Loss: 14791.1787109375, BCE Loss: 13656.8037109375, KLD Loss: 1134.37451171875\n",
      "Weights:  tensor([0.9531, 0.0469], dtype=torch.float64)\n",
      "Epoch: 8, Loss: 14798.09765625, BCE Loss: 13581.5888671875, KLD Loss: 1216.50830078125\n",
      "Weights:  tensor([0.9530, 0.0470], dtype=torch.float64)\n",
      "Epoch: 9, Loss: 14718.705078125, BCE Loss: 13507.8046875, KLD Loss: 1210.9012451171875\n",
      "Weights:  tensor([0.9418, 0.0582], dtype=torch.float64)\n",
      "Epoch: 10, Loss: 14591.4951171875, BCE Loss: 13455.8466796875, KLD Loss: 1135.64892578125\n",
      "Weights:  tensor([0.9137, 0.0863], dtype=torch.float64)\n",
      "Epoch: 11, Loss: 14515.5185546875, BCE Loss: 13425.6220703125, KLD Loss: 1089.89599609375\n",
      "Weights:  tensor([0.9515, 0.0485], dtype=torch.float64)\n",
      "Epoch: 12, Loss: 14526.01953125, BCE Loss: 13379.91015625, KLD Loss: 1146.1092529296875\n",
      "Weights:  tensor([0.9083, 0.0917], dtype=torch.float64)\n",
      "Epoch: 13, Loss: 14426.5849609375, BCE Loss: 13352.3134765625, KLD Loss: 1074.270751953125\n",
      "Weights:  tensor([0.9351, 0.0649], dtype=torch.float64)\n",
      "Epoch: 14, Loss: 14429.06640625, BCE Loss: 13315.9853515625, KLD Loss: 1113.0802001953125\n",
      "Weights:  tensor([0.9420, 0.0580], dtype=torch.float64)\n",
      "Epoch: 15, Loss: 14370.72265625, BCE Loss: 13236.1689453125, KLD Loss: 1134.554931640625\n",
      "Weights:  tensor([0.8322, 0.1678], dtype=torch.float64)\n",
      "Epoch: 16, Loss: 14259.912109375, BCE Loss: 13230.009765625, KLD Loss: 1029.90478515625\n",
      "Weights:  tensor([0.9293, 0.0707], dtype=torch.float64)\n",
      "Epoch: 17, Loss: 14329.1103515625, BCE Loss: 13225.677734375, KLD Loss: 1103.4315185546875\n",
      "Weights:  tensor([0.9061, 0.0939], dtype=torch.float64)\n",
      "Epoch: 18, Loss: 14267.4375, BCE Loss: 13203.6533203125, KLD Loss: 1063.783935546875\n",
      "Weights:  tensor([0.9180, 0.0820], dtype=torch.float64)\n",
      "Epoch: 19, Loss: 14318.7626953125, BCE Loss: 13262.2939453125, KLD Loss: 1056.46826171875\n",
      "Weights:  tensor([0.9206, 0.0794], dtype=torch.float64)\n",
      "Epoch: 20, Loss: 14278.1376953125, BCE Loss: 13171.7626953125, KLD Loss: 1106.3763427734375\n",
      "Weights:  tensor([0.9394, 0.0606], dtype=torch.float64)\n",
      "Epoch: 21, Loss: 14236.2509765625, BCE Loss: 13136.3701171875, KLD Loss: 1099.8809814453125\n",
      "Weights:  tensor([0.9501, 0.0499], dtype=torch.float64)\n",
      "Epoch: 22, Loss: 14258.90234375, BCE Loss: 13138.9296875, KLD Loss: 1119.97314453125\n",
      "Weights:  tensor([0.9530, 0.0470], dtype=torch.float64)\n",
      "Epoch: 23, Loss: 14248.724609375, BCE Loss: 13130.611328125, KLD Loss: 1118.1131591796875\n",
      "Weights:  tensor([0.8794, 0.1206], dtype=torch.float64)\n",
      "Epoch: 24, Loss: 14204.1728515625, BCE Loss: 13128.6748046875, KLD Loss: 1075.4984130859375\n",
      "Weights:  tensor([0.9582, 0.0418], dtype=torch.float64)\n",
      "Epoch: 25, Loss: 14351.7939453125, BCE Loss: 13188.1025390625, KLD Loss: 1163.69189453125\n",
      "Weights:  tensor([0.9098, 0.0902], dtype=torch.float64)\n",
      "Epoch: 26, Loss: 14126.3076171875, BCE Loss: 13062.8125, KLD Loss: 1063.4964599609375\n",
      "Weights:  tensor([0.8358, 0.1642], dtype=torch.float64)\n",
      "Epoch: 27, Loss: 14070.525390625, BCE Loss: 13061.2333984375, KLD Loss: 1009.2901611328125\n",
      "Weights:  tensor([0.9352, 0.0648], dtype=torch.float64)\n",
      "Epoch: 28, Loss: 14161.294921875, BCE Loss: 13024.6923828125, KLD Loss: 1136.6019287109375\n",
      "Weights:  tensor([0.8784, 0.1216], dtype=torch.float64)\n",
      "Epoch: 29, Loss: 14113.9921875, BCE Loss: 13086.224609375, KLD Loss: 1027.767578125\n"
     ]
    }
   ],
   "source": [
    "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt\n",
    "\n",
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Change 1: Create a SoftAdapt object (with your desired variant)\n",
    "softadapt_object = LossWeightedSoftAdapt(beta=0.001)\n",
    "\n",
    "# Change 2: Define how often SoftAdapt calculate weights for the loss components\n",
    "epochs_to_make_updates = 5\n",
    "\n",
    "values_of_component_1 = []\n",
    "values_of_component_2 = []\n",
    "# Initializing adaptive weights to all ones.\n",
    "adapt_weights = torch.tensor([1,1])\n",
    "\n",
    "limit = 101\n",
    "\n",
    "count = 0\n",
    "\n",
    "for current_epoch in range(1, 30):\n",
    "    for x, y in mnist_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        count += 1\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "\n",
    "        bce_loss = loss['BCE_loss']\n",
    "        kld = loss['KLD_loss']\n",
    "\n",
    "        values_of_component_1.append(bce_loss)\n",
    "        values_of_component_2.append(kld)\n",
    "\n",
    "        if (current_epoch % epochs_to_make_updates == 0 and current_epoch > 1 and count >= limit) or count >= limit:\n",
    "            # Change 3: Update weights of components\n",
    "            count = 0\n",
    "            # print(\"Adaptive weights: \", adapt_weights)\n",
    "            # print(\"epoch\")\n",
    "            # print(current_epoch)\n",
    "            first = torch.tensor(values_of_component_1, dtype=torch.float64)\n",
    "            second = torch.tensor(values_of_component_2, dtype=torch.float64)\n",
    "            # print(first)\n",
    "            # print(second)\n",
    "            # print(first.dtype)\n",
    "            # print(second.dtype)\n",
    "            # print(first.shape)\n",
    "            # print(second.shape)\n",
    "            adapt_weights = softadapt_object.get_component_weights(first, second,verbose=False)\n",
    "            #print(\"WORKS\")\n",
    "                                           \n",
    "        \n",
    "            # Resetting the lists to start fresh (this part is optional)\n",
    "            values_of_component_1 = []\n",
    "            values_of_component_2 = []\n",
    "\n",
    "        loss = adapt_weights[0] * bce_loss + adapt_weights[1] * kld\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        bce_losses = []\n",
    "        kld_losses = []\n",
    "\n",
    "        for x, y in val_dataloader:\n",
    "            x_recon, mean, log_var = model(x, y)\n",
    "            loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "            losses.append(loss['loss'])\n",
    "            bce_losses.append(loss['BCE_loss'])\n",
    "            kld_losses.append(loss['KLD_loss'])\n",
    "        print(\"Weights: \", adapt_weights)\n",
    "        print(\"Epoch: {}, Loss: {}, BCE Loss: {}, KLD Loss: {}\".format(current_epoch, torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(bce_losses)), torch.mean(torch.tensor(kld_losses))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
