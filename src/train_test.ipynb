{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim: int, h_dim1: int, h_dim2: int, z_dim: int):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "    def loss(self, batch, outputs):\n",
    "        x, y = batch\n",
    "        x_recon, mean, log_var  = outputs\n",
    "\n",
    "        BCE = recon_loss(x_recon, x.view(-1, 784))\n",
    "        KLD = kl_loss(mean, log_var)\n",
    "        \n",
    "        loss = BCE + KLD\n",
    "\n",
    "        return { 'loss': loss, 'recon_loss_0': BCE, 'kl_loss': KLD}\n",
    "    \n",
    "def kl_loss(z_mean, z_log_var):\n",
    "        return -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    \n",
    "def recon_loss(inputs, outputs):\n",
    "    return F.mse_loss(inputs, outputs, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_extensions import BaseModule\n",
    "\n",
    "class VAEModule(BaseModule):\n",
    "    def __init__(self):\n",
    "        model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "        super().__init__(model)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.model(x, y)\n",
    "\n",
    "    def step(self, batch, batch_idx, mode = 'train'):\n",
    "        x, y = batch\n",
    "        x_hat = self(x, y)\n",
    "        loss = self.model.loss(batch, x_hat)\n",
    "        self.log_dict({f\"{mode}_{key}\": val.item() for key, val in loss.items()}, sync_dist=True, prog_bar=True)\n",
    "        return loss['loss']\n",
    "\n",
    "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt\n",
    "class VAEModuleSoftAdapt(BaseModule):\n",
    "    def __init__(self):\n",
    "        model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "        self.softadapt_object = LossWeightedSoftAdapt(beta=0.001)\n",
    "        self.reconstruction_losses = []\n",
    "        self.kl_losses = []\n",
    "        self.adapt_weights = torch.tensor([1,1])\n",
    "        super().__init__(model)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.model(x, y)\n",
    "\n",
    "    def step(self, batch, batch_idx, mode = 'train'):\n",
    "        x, y = batch\n",
    "        x_hat = self(x, y)\n",
    "        loss = self.model.loss(batch, x_hat)\n",
    "        recon = loss['recon_loss_0']\n",
    "        kl = loss['kl_loss']\n",
    "\n",
    "        if mode == 'train':\n",
    "            copy_recon = recon.detach().clone()\n",
    "            copy_kl = kl.detach().clone()\n",
    "            self.reconstruction_losses.append(copy_recon)\n",
    "            self.kl_losses.append(copy_kl)\n",
    "\n",
    "        if len(self.reconstruction_losses) > 100 and mode == 'train':\n",
    "            first = torch.tensor(self.reconstruction_losses, dtype=torch.float64)\n",
    "            second = torch.tensor(self.kl_losses, dtype=torch.float64)\n",
    "\n",
    "            self.adapt_weights = self.softadapt_object.get_component_weights(first, second, verbose=False)\n",
    "\n",
    "            self.reconstruction_losses = []\n",
    "            self.kl_losses = []\n",
    "\n",
    "        self.log_dict({f\"{mode}_{key}\": val.item() for key, val in loss.items()}, prog_bar=True)\n",
    "\n",
    "        return self.adapt_weights[0]  * recon + self.adapt_weights[1] * kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/pyt ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medvardsz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231212_114125-c09j75st</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/c09j75st' target=\"_blank\">VAE-simple</a></strong> to <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/c09j75st' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/c09j75st</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /Home/siv34/edzak2974/projects/MastersThesis/src/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | VAE  | 1.1 M \n",
      "-------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.275     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 600/600 [00:08<00:00, 73.96it/s, v_num=75st, train_loss=3.53e+3, train_recon_loss_0=3e+3, train_kl_loss=536.0, val_loss=3.43e+3, val_recon_loss_0=2.89e+3, val_kl_loss=537.0]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 600/600 [00:08<00:00, 73.94it/s, v_num=75st, train_loss=3.53e+3, train_recon_loss_0=3e+3, train_kl_loss=536.0, val_loss=3.43e+3, val_recon_loss_0=2.89e+3, val_kl_loss=537.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">VAE-simple</strong> at: <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/c09j75st' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/c09j75st</a><br/>Synced 6 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning_extensions import ExtendedTrainer\n",
    "\n",
    "model = VAEModule()\n",
    "model_name = \"VAE-simple\"\n",
    "trainer = ExtendedTrainer(project_name=\"MTVAEs_SoftAdapt\", max_epochs=30, model_name=model_name)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/pyt ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medvardsz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231212_120943-ddbklk5g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/ddbklk5g' target=\"_blank\">VAE-softadapt</a></strong> to <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/ddbklk5g' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/ddbklk5g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /Home/siv34/edzak2974/projects/MastersThesis/src/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | VAE  | 1.1 M \n",
      "-------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.275     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 600/600 [00:08<00:00, 71.30it/s, v_num=lk5g, train_loss=3.48e+3, train_recon_loss_0=2.75e+3, train_kl_loss=731.0, val_loss=3.5e+3, val_recon_loss_0=2.76e+3, val_kl_loss=740.0] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 600/600 [00:08<00:00, 71.28it/s, v_num=lk5g, train_loss=3.48e+3, train_recon_loss_0=2.75e+3, train_kl_loss=731.0, val_loss=3.5e+3, val_recon_loss_0=2.76e+3, val_kl_loss=740.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">VAE-softadapt</strong> at: <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/ddbklk5g' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/ddbklk5g</a><br/>Synced 6 W&B file(s), 0 media file(s), 17 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning_extensions import ExtendedTrainer\n",
    "\n",
    "model = VAEModuleSoftAdapt()\n",
    "model_name = \"VAE-softadapt\"\n",
    "trainer = ExtendedTrainer(project_name=\"MTVAEs_SoftAdapt\", max_epochs=30, model_name=model_name)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  tensor([1, 1])\n",
      "Epoch: 1, Loss: 4177.669921875, Recon Loss: 3823.641357421875, KL Loss: 354.0292053222656\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 2, Loss: 3928.5625, Recon Loss: 3505.71875, KL Loss: 422.8433837890625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 3, Loss: 3830.398193359375, Recon Loss: 3402.633056640625, KL Loss: 427.7652282714844\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 4, Loss: 3739.007568359375, Recon Loss: 3274.516357421875, KL Loss: 464.4911804199219\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 5, Loss: 3709.9306640625, Recon Loss: 3242.3603515625, KL Loss: 467.5703125\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 6, Loss: 3667.258056640625, Recon Loss: 3202.8046875, KL Loss: 464.4531555175781\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 7, Loss: 3654.521240234375, Recon Loss: 3198.266357421875, KL Loss: 456.2549133300781\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 8, Loss: 3625.47412109375, Recon Loss: 3150.82958984375, KL Loss: 474.64422607421875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 9, Loss: 3601.628173828125, Recon Loss: 3099.199951171875, KL Loss: 502.42840576171875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 10, Loss: 3592.939697265625, Recon Loss: 3090.4208984375, KL Loss: 502.51873779296875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 11, Loss: 3579.022705078125, Recon Loss: 3076.6494140625, KL Loss: 502.3739013671875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 12, Loss: 3579.0322265625, Recon Loss: 3077.261474609375, KL Loss: 501.7710266113281\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 13, Loss: 3565.087158203125, Recon Loss: 3044.984375, KL Loss: 520.10302734375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 14, Loss: 3548.794921875, Recon Loss: 3041.0546875, KL Loss: 507.7402648925781\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 15, Loss: 3553.133056640625, Recon Loss: 3045.095703125, KL Loss: 508.0374755859375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 16, Loss: 3525.41748046875, Recon Loss: 3012.59130859375, KL Loss: 512.8259887695312\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 17, Loss: 3527.480224609375, Recon Loss: 3008.5849609375, KL Loss: 518.8951416015625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 18, Loss: 3529.916259765625, Recon Loss: 3006.909912109375, KL Loss: 523.0056762695312\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 19, Loss: 3514.84033203125, Recon Loss: 2991.859619140625, KL Loss: 522.9810791015625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 20, Loss: 3519.542724609375, Recon Loss: 3005.798828125, KL Loss: 513.744384765625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 21, Loss: 3496.987548828125, Recon Loss: 2980.703125, KL Loss: 516.2844848632812\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 22, Loss: 3506.51123046875, Recon Loss: 2974.644287109375, KL Loss: 531.8667602539062\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 23, Loss: 3507.7158203125, Recon Loss: 2974.46875, KL Loss: 533.2474365234375\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 24, Loss: 3483.327880859375, Recon Loss: 2957.431640625, KL Loss: 525.8961181640625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 25, Loss: 3482.302490234375, Recon Loss: 2958.198974609375, KL Loss: 524.103515625\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 26, Loss: 3483.862548828125, Recon Loss: 2963.396240234375, KL Loss: 520.4664916992188\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 27, Loss: 3476.849365234375, Recon Loss: 2950.4013671875, KL Loss: 526.4482421875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 28, Loss: 3510.99462890625, Recon Loss: 2977.781494140625, KL Loss: 533.2137451171875\n",
      "Weights:  tensor([1, 1])\n",
      "Epoch: 29, Loss: 3470.962890625, Recon Loss: 2932.5849609375, KL Loss: 538.3780517578125\n"
     ]
    }
   ],
   "source": [
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for current_epoch in range(1, 30):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        bce_losses = []\n",
    "        kld_losses = []\n",
    "\n",
    "        for x, y in val_loader:\n",
    "            x_recon, mean, log_var = model(x, y)\n",
    "            loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "            losses.append(loss['loss'])\n",
    "            bce_losses.append(loss['recon_loss_0'])\n",
    "            kld_losses.append(loss['kl_loss'])\n",
    "        print(\"Weights: \", torch.tensor([1,1]))\n",
    "        print(\"Epoch: {}, Loss: {}, Recon Loss: {}, KL Loss: {}\".format(current_epoch, torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(bce_losses)), torch.mean(torch.tensor(kld_losses))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train example with softadapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Home/siv34/edzak2974/projects/MastersThesis/src/wandb/run-20231212_101534-kkij7482</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/kkij7482' target=\"_blank\">VAE-softadapt-custom</a></strong> to <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/kkij7482' target=\"_blank\">https://wandb.ai/edvardsz/MTVAEs_SoftAdapt/runs/kkij7482</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  tensor([0.8314, 0.1686], dtype=torch.float64)\n",
      "Epoch: 1, Loss: 4298.5791015625, Recon Loss: 3638.10595703125, KL Loss: 660.4732666015625\n",
      "Weights:  tensor([0.8357, 0.1643], dtype=torch.float64)\n",
      "Epoch: 2, Loss: 4009.738525390625, Recon Loss: 3350.79150390625, KL Loss: 658.9473266601562\n",
      "Weights:  tensor([0.8220, 0.1780], dtype=torch.float64)\n",
      "Epoch: 3, Loss: 3932.1123046875, Recon Loss: 3269.55126953125, KL Loss: 662.5613403320312\n",
      "Weights:  tensor([0.8353, 0.1647], dtype=torch.float64)\n",
      "Epoch: 4, Loss: 3866.88134765625, Recon Loss: 3165.371826171875, KL Loss: 701.5097045898438\n",
      "Weights:  tensor([0.7787, 0.2213], dtype=torch.float64)\n",
      "Epoch: 5, Loss: 3794.3525390625, Recon Loss: 3155.98779296875, KL Loss: 638.365234375\n",
      "Weights:  tensor([0.8174, 0.1826], dtype=torch.float64)\n",
      "Epoch: 6, Loss: 3740.163818359375, Recon Loss: 3067.777587890625, KL Loss: 672.3861694335938\n",
      "Weights:  tensor([0.8183, 0.1817], dtype=torch.float64)\n",
      "Epoch: 7, Loss: 3714.518798828125, Recon Loss: 3017.746826171875, KL Loss: 696.7726440429688\n",
      "Weights:  tensor([0.7990, 0.2010], dtype=torch.float64)\n",
      "Epoch: 8, Loss: 3696.24462890625, Recon Loss: 3004.462158203125, KL Loss: 691.7821655273438\n",
      "Weights:  tensor([0.7986, 0.2014], dtype=torch.float64)\n",
      "Epoch: 9, Loss: 3652.739013671875, Recon Loss: 2971.585205078125, KL Loss: 681.15380859375\n",
      "Weights:  tensor([0.8101, 0.1899], dtype=torch.float64)\n",
      "Epoch: 10, Loss: 3629.656005859375, Recon Loss: 2953.738525390625, KL Loss: 675.91748046875\n",
      "Weights:  tensor([0.7485, 0.2515], dtype=torch.float64)\n",
      "Epoch: 11, Loss: 3600.1611328125, Recon Loss: 2924.15869140625, KL Loss: 676.00244140625\n",
      "Weights:  tensor([0.8086, 0.1914], dtype=torch.float64)\n",
      "Epoch: 12, Loss: 3621.46728515625, Recon Loss: 2920.011474609375, KL Loss: 701.4556274414062\n",
      "Weights:  tensor([0.8501, 0.1499], dtype=torch.float64)\n",
      "Epoch: 13, Loss: 3648.84521484375, Recon Loss: 2913.521240234375, KL Loss: 735.3242797851562\n",
      "Weights:  tensor([0.8177, 0.1823], dtype=torch.float64)\n",
      "Epoch: 14, Loss: 3617.089111328125, Recon Loss: 2897.77197265625, KL Loss: 719.3171997070312\n",
      "Weights:  tensor([0.7895, 0.2105], dtype=torch.float64)\n",
      "Epoch: 15, Loss: 3565.216552734375, Recon Loss: 2895.0634765625, KL Loss: 670.153564453125\n",
      "Weights:  tensor([0.7867, 0.2133], dtype=torch.float64)\n",
      "Epoch: 16, Loss: 3574.5068359375, Recon Loss: 2874.649658203125, KL Loss: 699.8571166992188\n",
      "Weights:  tensor([0.8190, 0.1810], dtype=torch.float64)\n",
      "Epoch: 17, Loss: 3580.896484375, Recon Loss: 2866.732177734375, KL Loss: 714.1643676757812\n",
      "Weights:  tensor([0.8238, 0.1762], dtype=torch.float64)\n",
      "Epoch: 18, Loss: 3584.919921875, Recon Loss: 2854.81787109375, KL Loss: 730.1021728515625\n",
      "Weights:  tensor([0.7898, 0.2102], dtype=torch.float64)\n",
      "Epoch: 19, Loss: 3546.695556640625, Recon Loss: 2851.162841796875, KL Loss: 695.53271484375\n",
      "Weights:  tensor([0.7689, 0.2311], dtype=torch.float64)\n",
      "Epoch: 20, Loss: 3578.684326171875, Recon Loss: 2864.29150390625, KL Loss: 714.392822265625\n",
      "Weights:  tensor([0.7981, 0.2019], dtype=torch.float64)\n",
      "Epoch: 21, Loss: 3568.24462890625, Recon Loss: 2840.685302734375, KL Loss: 727.5596923828125\n",
      "Weights:  tensor([0.7966, 0.2034], dtype=torch.float64)\n",
      "Epoch: 22, Loss: 3557.2275390625, Recon Loss: 2827.9892578125, KL Loss: 729.23828125\n",
      "Weights:  tensor([0.7762, 0.2238], dtype=torch.float64)\n",
      "Epoch: 23, Loss: 3538.5234375, Recon Loss: 2843.569091796875, KL Loss: 694.95458984375\n",
      "Weights:  tensor([0.8041, 0.1959], dtype=torch.float64)\n",
      "Epoch: 24, Loss: 3548.908203125, Recon Loss: 2818.064697265625, KL Loss: 730.84375\n",
      "Weights:  tensor([0.8080, 0.1920], dtype=torch.float64)\n",
      "Epoch: 25, Loss: 3577.2275390625, Recon Loss: 2836.065673828125, KL Loss: 741.1619262695312\n",
      "Weights:  tensor([0.8001, 0.1999], dtype=torch.float64)\n",
      "Epoch: 26, Loss: 3539.666259765625, Recon Loss: 2821.54443359375, KL Loss: 718.1220092773438\n",
      "Weights:  tensor([0.7847, 0.2153], dtype=torch.float64)\n",
      "Epoch: 27, Loss: 3514.827880859375, Recon Loss: 2800.389404296875, KL Loss: 714.4382934570312\n",
      "Weights:  tensor([0.7821, 0.2179], dtype=torch.float64)\n",
      "Epoch: 28, Loss: 3535.90869140625, Recon Loss: 2823.333740234375, KL Loss: 712.5750122070312\n",
      "Weights:  tensor([0.7663, 0.2337], dtype=torch.float64)\n",
      "Epoch: 29, Loss: 3504.138671875, Recon Loss: 2796.555419921875, KL Loss: 707.583740234375\n"
     ]
    }
   ],
   "source": [
    "from softadapt import SoftAdapt, NormalizedSoftAdapt, LossWeightedSoftAdapt\n",
    "import wandb\n",
    "\n",
    "model = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Change 1: Create a SoftAdapt object (with your desired variant)\n",
    "softadapt_object = LossWeightedSoftAdapt(beta=0.001)\n",
    "\n",
    "# Change 2: Define how often SoftAdapt calculate weights for the loss components\n",
    "epochs_to_make_updates = 5\n",
    "\n",
    "values_of_component_1 = []\n",
    "values_of_component_2 = []\n",
    "# Initializing adaptive weights to all ones.\n",
    "adapt_weights = torch.tensor([1,1])\n",
    "\n",
    "limit = 101\n",
    "\n",
    "count = 0\n",
    "\n",
    "for current_epoch in range(1, 30):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        count += 1\n",
    "        x_recon, mean, log_var = model(x, y)\n",
    "        loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "\n",
    "        bce_loss = loss['recon_loss_0']\n",
    "        kld = loss['kl_loss']\n",
    "\n",
    "        values_of_component_1.append(bce_loss)\n",
    "        values_of_component_2.append(kld)\n",
    "\n",
    "        if (current_epoch % epochs_to_make_updates == 0 and current_epoch > 1 and count >= limit) or count >= limit:\n",
    "            # Change 3: Update weights of components\n",
    "            count = 0\n",
    "            # print(\"Adaptive weights: \", adapt_weights)\n",
    "            # print(\"epoch\")\n",
    "            # print(current_epoch)\n",
    "            first = torch.tensor(values_of_component_1, dtype=torch.float64)\n",
    "            second = torch.tensor(values_of_component_2, dtype=torch.float64)\n",
    "            # print(first)\n",
    "            # print(second)\n",
    "            # print(first.dtype)\n",
    "            # print(second.dtype)\n",
    "            # print(first.shape)\n",
    "            # print(second.shape)\n",
    "            adapt_weights = softadapt_object.get_component_weights(first, second,verbose=False)\n",
    "            #print(\"WORKS\")\n",
    "                                           \n",
    "        \n",
    "            # Resetting the lists to start fresh (this part is optional)\n",
    "            values_of_component_1 = []\n",
    "            values_of_component_2 = []\n",
    "\n",
    "        loss = adapt_weights[0] * bce_loss + adapt_weights[1] * kld\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        bce_losses = []\n",
    "        kld_losses = []\n",
    "\n",
    "        for x, y in val_loader:\n",
    "            x_recon, mean, log_var = model(x, y)\n",
    "            loss = model.loss((x, y), (x_recon, mean, log_var))\n",
    "            losses.append(loss['loss'])\n",
    "            bce_losses.append(loss['recon_loss_0'])\n",
    "            kld_losses.append(loss['kl_loss'])\n",
    "        print(\"Weights: \", adapt_weights)\n",
    "        print(\"Epoch: {}, Loss: {}, Recon Loss: {}, KL Loss: {}\".format(current_epoch, torch.mean(torch.tensor(losses)), torch.mean(torch.tensor(bce_losses)), torch.mean(torch.tensor(kld_losses))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
