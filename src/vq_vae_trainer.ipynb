{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_mnist, get_observation_pixels\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_loader, test_loader, val_loader = load_mnist(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VQVAE\n",
    "\n",
    "class VQVAE_trainer(nn.Module):\n",
    "    def __init__(self, in_channels, num_embeddings, embedding_dim):\n",
    "        super(VQVAE_trainer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.encoder = Encoder(in_channels, embedding_dim)\n",
    "\n",
    "        self.codebook = VectorQuantizer(num_embeddings, embedding_dim)\n",
    "\n",
    "        self.decoder = Decoder(in_channels, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, C, H, W)\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        z = z.permute(0, 2, 3, 1) # (B, C, H, W) -> (B, H, W, C)\n",
    "        z, embedding_indices, loss = self.codebook(z)\n",
    "        z = z.permute(0, 3, 1, 2) # (B, H, W, C) -> (B, C, H, W)\n",
    "\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        return x_hat, z, embedding_indices, loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
