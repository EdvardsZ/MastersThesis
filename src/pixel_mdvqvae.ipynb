{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, get_observation_pixels\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "from config import load_config, get_model_name\n",
    "config = load_config(\"PixelMDVQVAE\")\n",
    "\n",
    "train_loader, test_loader, val_loader = load_dataset(config[\"data_params\"])\n",
    "\n",
    "model_name = get_model_name(config)\n",
    "\n",
    "from models import PixelMDVQVAE\n",
    "\n",
    "model = PixelMDVQVAE(**config[\"model_params\"])\n",
    "\n",
    "randn = torch.randn(32, 1, 28, 28)\n",
    "\n",
    "output, _, _, _ = model(randn, randn, randn)\n",
    "\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/pyt ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: lightning_logs/PixelMDVQVAE(128_16)?dataset=MNIST&batch_size=128&max_epochs=50\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medvardsz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231023_150932-28ekc9vk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders/runs/28ekc9vk' target=\"_blank\">PixelMDVQVAE(128_16)?dataset=MNIST&batch_size=128&max_epochs=50</a></strong> to <a href='https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders' target=\"_blank\">https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders/runs/28ekc9vk' target=\"_blank\">https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders/runs/28ekc9vk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /Home/siv34/edzak2974/projects/MastersThesis/src/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/Home/siv34/edzak2974/.conda/envs/pytorch2.1/lib/python3.10/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:452: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | PixelMDVQVAE | 13.8 M\n",
      "---------------------------------------\n",
      "13.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.8 M    Total params\n",
      "55.263    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 469/469 [00:10<00:00, 46.29it/s, v_num=c9vk, train_loss=0.310, train_recon_loss=0.052, train_recon_loss_2=0.0512, train_vq_loss=0.207, train_commitment_loss=0.138, train_embeddding_loss=0.138, val_loss=0.289, val_recon_loss=0.0505, val_recon_loss_2=0.0489, val_vq_loss=0.190, val_commitment_loss=0.126, val_embeddding_loss=0.126] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 469/469 [00:10<00:00, 46.27it/s, v_num=c9vk, train_loss=0.310, train_recon_loss=0.052, train_recon_loss_2=0.0512, train_vq_loss=0.207, train_commitment_loss=0.138, train_embeddding_loss=0.138, val_loss=0.289, val_recon_loss=0.0505, val_recon_loss_2=0.0489, val_vq_loss=0.190, val_commitment_loss=0.126, val_embeddding_loss=0.126]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_commitment_loss</td><td>▇█▇▄▄▃▃▃▃▄▃▄▃▂▂▂▂▃▁▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>train_embeddding_loss</td><td>▇█▇▄▄▃▃▃▃▄▃▄▃▂▂▂▂▃▁▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>train_loss</td><td>█▇▇▄▄▃▃▃▃▄▃▃▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>train_recon_loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train_recon_loss_2</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_vq_loss</td><td>▇█▇▄▄▃▃▃▃▄▃▄▃▂▂▂▂▃▁▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_commitment_loss</td><td>▅▃█▆▅▄▃▅▃▃▄▃▄▂▂▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_embeddding_loss</td><td>▅▃█▆▅▄▃▅▃▃▄▃▄▂▂▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▄█▆▅▄▄▄▃▃▄▃▄▂▂▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_recon_loss</td><td>█▅▆▅▃▄▃▃▂▃▃▂▂▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_recon_loss_2</td><td>█▅▆▄▃▄▃▃▂▃▃▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_vq_loss</td><td>▅▃█▆▅▄▃▅▃▃▄▃▄▂▂▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>train_commitment_loss</td><td>0.1379</td></tr><tr><td>train_embeddding_loss</td><td>0.1379</td></tr><tr><td>train_loss</td><td>0.31007</td></tr><tr><td>train_recon_loss</td><td>0.05201</td></tr><tr><td>train_recon_loss_2</td><td>0.05122</td></tr><tr><td>train_vq_loss</td><td>0.20685</td></tr><tr><td>trainer/global_step</td><td>23449</td></tr><tr><td>val_commitment_loss</td><td>0.1264</td></tr><tr><td>val_embeddding_loss</td><td>0.1264</td></tr><tr><td>val_loss</td><td>0.28898</td></tr><tr><td>val_recon_loss</td><td>0.05054</td></tr><tr><td>val_recon_loss_2</td><td>0.04885</td></tr><tr><td>val_vq_loss</td><td>0.1896</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PixelMDVQVAE(128_16)?dataset=MNIST&batch_size=128&max_epochs=50</strong> at: <a href='https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders/runs/28ekc9vk' target=\"_blank\">https://wandb.ai/edvardsz/MultiTaskVariationalAutoecnoders/runs/28ekc9vk</a><br/>Synced 6 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231023_150932-28ekc9vk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trainers import SuperTrainer, VAEModule\n",
    "\n",
    "model = VAEModule(config['model_params'], model_name=config['model_name'])\n",
    "\n",
    "from trainers import SuperTrainer\n",
    "trainer = SuperTrainer(**config['trainer_params'], model_name=model_name)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "#save\n",
    "trainer.save_model_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule.load_model_checkpoint(model_name)\n",
    "model.eval()\n",
    "\n",
    "from plotting import plot_samples_with_reconstruction\n",
    "\n",
    "plot_samples_with_reconstruction(model, next(iter(test_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
