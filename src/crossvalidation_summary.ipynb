{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results...\n",
      "Loaded:  45\n",
      "Done.\n",
      "********************************************************************************\n",
      "VQVAE(128_16_[32, 64]_0)?dataset=MNIST&batch_size=128&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1)\n",
      "Reconstruction loss: 0.0027908134274184705 +- 8.83935007784005e-09\n",
      "VQ objective loss: 0.010065646283328534 +- 4.669239887607202e-07\n",
      "********************************************************************************\n",
      "VQVAE(128_32_[128, 256]_2)?dataset=MNIST&batch_size=128&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2)\n",
      "Reconstruction loss: 0.0017001190455630422 +- 1.5770437576201424e-09\n",
      "VQ objective loss: 0.002214624173939228 +- 1.9928815230617174e-08\n",
      "********************************************************************************\n",
      "VQVAE(256_64_[128, 256]_4)?dataset=MNIST&batch_size=128&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3)\n",
      "Reconstruction loss: 0.001903897407464683 +- 1.3200110901672714e-08\n",
      "VQ objective loss: 0.002362433634698391 +- 4.271572894880543e-08\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_32_[128, 256]_2)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=60&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 0.0031485392712056635 +- 2.0581008736589996e-07\n",
      "VQ objective loss: 0.001257730694487691 +- 2.624289771123355e-07\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_32_[128, 256]_2)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=40&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 0.0031112424563616513 +- 1.5841399132076727e-07\n",
      "VQ objective loss: 0.0013913763395976275 +- 3.502900838928844e-07\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_32_[128, 256]_2)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=60&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 0.0035280059091746807 +- 3.3194033172677124e-07\n",
      "VQ objective loss: 0.001176433265209198 +- 4.3116723749055103e-07\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_32_[128, 256]_2)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=40&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 0.0028518489561975002 +- 1.357080907750559e-07\n",
      "VQ objective loss: 0.0016820409800857306 +- 5.5956246674285184e-08\n",
      "********************************************************************************\n",
      "SCVQVAE1D(256_64_[128, 256]_4)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=60&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Single Decoder method\n",
      "Reconstruction loss: 0.003695751167833805 +- 5.6850477279991346e-08\n",
      "VQ objective loss: 0.0004969128873199225 +- 1.1249271990578708e-07\n",
      "********************************************************************************\n",
      "SCVQVAE1D(256_64_[128, 256]_4)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=40&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Single Decoder method\n",
      "Reconstruction loss: 0.003781072935089469 +- 8.703529529088208e-08\n",
      "VQ objective loss: 0.0008301423193188384 +- 2.4480503773192496e-07\n",
      "********************************************************************************\n",
      "SCVQVAE1D(256_64_[128, 256]_4)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=60&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Single Decoder method\n",
      "Reconstruction loss: 0.003825067775323987 +- 2.3995112117332526e-08\n",
      "VQ objective loss: 0.0006776066380552948 +- 4.0133794003810154e-08\n",
      "********************************************************************************\n",
      "SCVQVAE1D(256_64_[128, 256]_4)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=40&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Single Decoder method\n",
      "Reconstruction loss: 0.003919941978529096 +- 1.0546566549932135e-07\n",
      "VQ objective loss: 0.0007108614721801132 +- 1.7938744690125024e-08\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_16_[32, 64]_0_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 0.012305403221398592 +- 0.00038526916826302573\n",
      "VQ objective loss: 0.003475161537062377 +- 2.3461766616996344e-06\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_16_[32, 64]_0)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 0.00245369290933013 +- 6.4834449898945955e-09\n",
      "VQ objective loss: 0.008168273605406284 +- 1.8744889561730926e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_16_[32, 64]_0_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 0.0024290796369314193 +- 3.5535142889724095e-08\n",
      "VQ objective loss: 0.004208790836855769 +- 2.38779463963357e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_16_[32, 64]_0)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 0.0022777654230594635 +- 2.372085286745165e-09\n",
      "VQ objective loss: 0.007555400673300028 +- 2.791041981990705e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_32_[128, 256]_2_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 0.015453119250014424 +- 0.0007909762582692482\n",
      "VQ objective loss: 0.0012828646693378686 +- 3.593786489459831e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_32_[128, 256]_2)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 0.0014423630898818373 +- 2.777162928653239e-09\n",
      "VQ objective loss: 0.0027575922198593617 +- 3.31053772751038e-08\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_32_[128, 256]_2_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 0.042219922761432827 +- 0.0011233595591203993\n",
      "VQ objective loss: 0.0008865430601872504 +- 3.190830902573125e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(128_32_[128, 256]_2)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 0.0014907277654856443 +- 2.7077186373928793e-09\n",
      "VQ objective loss: 0.002970190392807126 +- 2.481416306142695e-08\n",
      "********************************************************************************\n",
      "SCVQVAE2D(256_64_[128, 256]_4_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Multi Decoder method\n",
      "Reconstruction loss: 0.017328017647378145 +- 0.0006917906885886589\n",
      "VQ objective loss: 0.0011302805098239333 +- 2.1825778610860345e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(256_64_[128, 256]_4)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Multi Decoder method\n",
      "Reconstruction loss: 0.0016995357349514962 +- 8.401850527228185e-09\n",
      "VQ objective loss: 0.002789504127576947 +- 1.1939532475781222e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(256_64_[128, 256]_4_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Multi Decoder method\n",
      "Reconstruction loss: 0.017758493893779814 +- 0.0006484058762241738\n",
      "VQ objective loss: 0.0008854981446347665 +- 2.7751091254304534e-07\n",
      "********************************************************************************\n",
      "SCVQVAE2D(256_64_[128, 256]_4)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.3) with Multi Decoder method\n",
      "Reconstruction loss: 0.0016934871673583984 +- 2.3667703595093213e-08\n",
      "VQ objective loss: 0.0024072423111647367 +- 1.238123932824917e-07\n",
      "********************************************************************************\n",
      "VAE(16)?dataset=MNIST&batch_size=128&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1)\n",
      "Reconstruction loss: 1788.2714599609376 +- 118.30354796886445\n",
      "KL divergence loss: 1658.025927734375 +- 448.85416162967687\n",
      "********************************************************************************\n",
      "VAE(64)?dataset=MNIST&batch_size=128&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2)\n",
      "Reconstruction loss: 1821.01376953125 +- 872.9030274534227\n",
      "KL divergence loss: 1619.6564208984375 +- 944.308314934969\n",
      "********************************************************************************\n",
      "SCVAE1D(16)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=60&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 2039.8653076171875 +- 1735.3406658744811\n",
      "KL divergence loss: 1241.298974609375 +- 286.53819813370706\n",
      "********************************************************************************\n",
      "SCVAE1D(16)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=40&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 2052.0941650390623 +- 2671.349552652836\n",
      "KL divergence loss: 1261.0806396484375 +- 683.8973432338238\n",
      "********************************************************************************\n",
      "SCVAE1D(16)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=60&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 2022.635791015625 +- 2743.7005813515184\n",
      "KL divergence loss: 1479.9053466796875 +- 304.74155895471574\n",
      "********************************************************************************\n",
      "SCVAE1D(16)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=40&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 2021.7352783203125 +- 960.7092666780948\n",
      "KL divergence loss: 1484.5746337890625 +- 239.55989287257194\n",
      "********************************************************************************\n",
      "SCVAE1D(64)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=60&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 2078.709814453125 +- 2719.992805519104\n",
      "KL divergence loss: 1254.209716796875 +- 396.9097038984299\n",
      "********************************************************************************\n",
      "SCVAE1D(64)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=40&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 2097.6779296875 +- 5374.176256809235\n",
      "KL divergence loss: 1257.294677734375 +- 101.5845208466053\n",
      "********************************************************************************\n",
      "SCVAE1D(64)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=60&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 2045.3642333984376 +- 2770.649666466713\n",
      "KL divergence loss: 1491.2306640625 +- 536.2919679677486\n",
      "********************************************************************************\n",
      "SCVAE1D(64)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=40&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Single Decoder method\n",
      "Reconstruction loss: 2046.8917236328125 +- 3452.54008589983\n",
      "KL divergence loss: 1473.0288330078124 +- 437.6589505636692\n",
      "********************************************************************************\n",
      "SCVAE2D(16_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 1681.989990234375 +- 130.67998458743097\n",
      "KL divergence loss: 1862.62607421875 +- 1350.4212458932402\n",
      "********************************************************************************\n",
      "SCVAE2D(16)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 1459.19443359375 +- 234.4662777030468\n",
      "KL divergence loss: 2226.8251953125 +- 1212.6229831457138\n",
      "********************************************************************************\n",
      "SCVAE2D(16_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 1588.7294921875 +- 328.21035463809966\n",
      "KL divergence loss: 1960.9904296875 +- 391.9483363544941\n",
      "********************************************************************************\n",
      "SCVAE2D(16)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.1) with Multi Decoder method\n",
      "Reconstruction loss: 1340.0640625 +- 539.2302231156826\n",
      "KL divergence loss: 2346.429052734375 +- 2609.6408039951325\n",
      "********************************************************************************\n",
      "SCVAE2D(64_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 1680.5236572265626 +- 1134.9123502993584\n",
      "KL divergence loss: 1884.6349853515626 +- 376.1075103127956\n",
      "********************************************************************************\n",
      "SCVAE2D(64)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=EXACT&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 1482.9427001953125 +- 848.9834826982021\n",
      "KL divergence loss: 2202.46845703125 +- 756.7410291528702\n",
      "********************************************************************************\n",
      "SCVAE2D(64_SOFT)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 1583.5907958984376 +- 232.5107874596119\n",
      "KL divergence loss: 1957.5203369140625 +- 478.03244150042536\n",
      "********************************************************************************\n",
      "SCVAE2D(64)?dataset=MNIST&batch_size=128&count_sampling=EXACT&pixel_sampling=UNIFORM&max_epochs=100_crossval_results.pt\n",
      "Gaussian VAE(Conf. Nr.2) with Multi Decoder method\n",
      "Reconstruction loss: 1360.943603515625 +- 886.0163918077945\n",
      "KL divergence loss: 2353.841650390625 +- 2278.114756717682\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_16_[32, 64]_0)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=60&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 0.0029208300169557334 +- 7.158733632569354e-08\n",
      "VQ objective loss: 0.0028578664641827345 +- 1.2995592355755612e-07\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_16_[32, 64]_0)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=GAUSSIAN&exponent=40&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 0.00284298756159842 +- 7.389299963161086e-08\n",
      "VQ objective loss: 0.0028133205138146876 +- 1.9630468744742754e-07\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_16_[32, 64]_0)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=60&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 0.0029609094373881815 +- 7.242617336278205e-08\n",
      "VQ objective loss: 0.002419593045488 +- 2.3817800363966703e-08\n",
      "********************************************************************************\n",
      "SCVQVAE1D(128_16_[32, 64]_0)?dataset=MNIST&batch_size=128&count_sampling=POWER_LAW&pixel_sampling=UNIFORM&exponent=40&max_epochs=100_crossval_results.pt\n",
      "VQ-VAE(Conf. Nr.1) with Single Decoder method\n",
      "Reconstruction loss: 0.002856406755745411 +- 6.432870332727555e-08\n",
      "VQ objective loss: 0.0029704443179070948 +- 4.2211813101171793e-07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from crossval_result_loader import get_results, Result\n",
    "\n",
    "dataset = \"MNIST\"\n",
    "project_name = \"MTVAEs_05.01-cross-val\"\n",
    "directory = f\"assets/results/raw/{project_name}/{dataset}/\"\n",
    "\n",
    "def print_results(directory: str):\n",
    "    results: list[Result] = get_results(directory)  \n",
    "    for result in results: \n",
    "        print(\"*\"*80)\n",
    "        \n",
    "        print (result.filename)\n",
    "        \n",
    "        print(result.get_display_model_name())\n",
    "        \n",
    "        losses = result.get_unconditioned_losses()\n",
    "        \n",
    "        for key, value in losses.items():\n",
    "            print (f\"{key}: {value.average} +- {value.std}\")\n",
    "            \n",
    "print_results(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results...\n",
      "Loaded:  45\n",
      "Done.\n",
      "********************************************************************************\n",
      "\\begin{center}\n",
      "\\begin{tabular}{||c|c|c|c|c||}\n",
      "\\hline\n",
      " Model name & Configuration & Method & Reconstruction loss & VQ loss \\\\\n",
      "\\hline\n",
      "\\multirow{27}{*}{\\rotatebox[origin=c]{90}{VQVAE}} & \\multirow{9}{*}{1} & VQ-VAE(Conf. Nr.1) & 0.0027908134274184705 +- 8.83935007784005e-09 & 0.010065646283328534 +- 4.669239887607202e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Multi Decoder method & 0.012305403221398592 +- 0.00038526916826302573 & 0.003475161537062377 +- 2.3461766616996344e-06 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Multi Decoder method & 0.00245369290933013 +- 6.4834449898945955e-09 & 0.008168273605406284 +- 1.8744889561730926e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Multi Decoder method & 0.0024290796369314193 +- 3.5535142889724095e-08 & 0.004208790836855769 +- 2.38779463963357e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Multi Decoder method & 0.0022777654230594635 +- 2.372085286745165e-09 & 0.007555400673300028 +- 2.791041981990705e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Single Decoder method & 0.0029208300169557334 +- 7.158733632569354e-08 & 0.0028578664641827345 +- 1.2995592355755612e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Single Decoder method & 0.00284298756159842 +- 7.389299963161086e-08 & 0.0028133205138146876 +- 1.9630468744742754e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Single Decoder method & 0.0029609094373881815 +- 7.242617336278205e-08 & 0.002419593045488 +- 2.3817800363966703e-08 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.1) with Single Decoder method & 0.002856406755745411 +- 6.432870332727555e-08 & 0.0029704443179070948 +- 4.2211813101171793e-07 \\\\\n",
      "\\cline{3-4}\n",
      " & \\multirow{9}{*}{2} & VQ-VAE(Conf. Nr.2) & 0.0017001190455630422 +- 1.5770437576201424e-09 & 0.002214624173939228 +- 1.9928815230617174e-08 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Single Decoder method & 0.0031485392712056635 +- 2.0581008736589996e-07 & 0.001257730694487691 +- 2.624289771123355e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Single Decoder method & 0.0031112424563616513 +- 1.5841399132076727e-07 & 0.0013913763395976275 +- 3.502900838928844e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Single Decoder method & 0.0035280059091746807 +- 3.3194033172677124e-07 & 0.001176433265209198 +- 4.3116723749055103e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Single Decoder method & 0.0028518489561975002 +- 1.357080907750559e-07 & 0.0016820409800857306 +- 5.5956246674285184e-08 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Multi Decoder method & 0.015453119250014424 +- 0.0007909762582692482 & 0.0012828646693378686 +- 3.593786489459831e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Multi Decoder method & 0.0014423630898818373 +- 2.777162928653239e-09 & 0.0027575922198593617 +- 3.31053772751038e-08 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Multi Decoder method & 0.042219922761432827 +- 0.0011233595591203993 & 0.0008865430601872504 +- 3.190830902573125e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.2) with Multi Decoder method & 0.0014907277654856443 +- 2.7077186373928793e-09 & 0.002970190392807126 +- 2.481416306142695e-08 \\\\\n",
      "\\cline{3-4}\n",
      " & \\multirow{9}{*}{3} & VQ-VAE(Conf. Nr.3) & 0.001903897407464683 +- 1.3200110901672714e-08 & 0.002362433634698391 +- 4.271572894880543e-08 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Single Decoder method & 0.003695751167833805 +- 5.6850477279991346e-08 & 0.0004969128873199225 +- 1.1249271990578708e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Single Decoder method & 0.003781072935089469 +- 8.703529529088208e-08 & 0.0008301423193188384 +- 2.4480503773192496e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Single Decoder method & 0.003825067775323987 +- 2.3995112117332526e-08 & 0.0006776066380552948 +- 4.0133794003810154e-08 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Single Decoder method & 0.003919941978529096 +- 1.0546566549932135e-07 & 0.0007108614721801132 +- 1.7938744690125024e-08 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Multi Decoder method & 0.017328017647378145 +- 0.0006917906885886589 & 0.0011302805098239333 +- 2.1825778610860345e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Multi Decoder method & 0.0016995357349514962 +- 8.401850527228185e-09 & 0.002789504127576947 +- 1.1939532475781222e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Multi Decoder method & 0.017758493893779814 +- 0.0006484058762241738 & 0.0008854981446347665 +- 2.7751091254304534e-07 \\\\\n",
      "\\cline{3-4}\n",
      " &  & VQ-VAE(Conf. Nr.3) with Multi Decoder method & 0.0016934871673583984 +- 2.3667703595093213e-08 & 0.0024072423111647367 +- 1.238123932824917e-07 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def len_vq_vae(results: list[Result]) -> int:\n",
    "    return len([result for result in results if \"VQVAE\" in result.filename])\n",
    "\n",
    "def len_vae(results: list[Result]) -> int:\n",
    "    return len([result for result in results if \"VAE\" in result.filename or \"SCVAE1D\" in result.filename or \"SCVAE2D\" in result.filename])\n",
    "\n",
    "def len_config_vq(results: list[Result], config: int) -> int:\n",
    "    return len([result for result in results if result.get_config_number() == config and \"VQVAE\" in result.filename])\n",
    "\n",
    "def conf_output(results: list[Result], conf: int, previous_conf: int | None) -> str:\n",
    "    if previous_conf == None:\n",
    "        return \"\\multirow{\"+ str(len_config_vq(results, conf)) +\"}{*}{\" + str(conf) + \"}\"\n",
    "    elif previous_conf != conf:\n",
    "        return \"\\multirow{\"+ str(len_config_vq(results, conf)) +\"}{*}{\" + str(conf) + \"}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "def output_latex_table_of_results(directory: str):\n",
    "    results : list[Result] = get_results(directory)\n",
    "    print(\"*\"*80)\n",
    "    print(\"\\\\begin{center}\")\n",
    "    print(\"\\\\begin{tabular}{||c|c|c|c|c||}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\" Model name & Configuration & Method & Reconstruction loss & VQ loss \\\\\\\\\")\n",
    "    print(\"\\\\hline\")\n",
    "    \n",
    "    \n",
    "    # sort by config number\n",
    "    results = sorted(results, key = lambda x: x.get_config_number())\n",
    "    \n",
    "    first_vq = \"\\multirow{\"+ str(len_vq_vae(results)) +\"}{*}{\\\\rotatebox[origin=c]{90}{VQVAE}}\" \n",
    "    \n",
    "    previous_conf = None\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        if \"VQVAE\" in result.filename:\n",
    "            losses = result.get_unconditioned_losses()\n",
    "            conf = result.get_config_number()\n",
    "            output_line = first_vq + \" & \" + conf_output(results, conf, previous_conf) + \" & \" + result.get_display_model_name() + \" & \"\n",
    "            \n",
    "            previous_conf = conf\n",
    "            \n",
    "            if first_vq != \"\":\n",
    "                first_vq = \"\"\n",
    "                \n",
    "            \n",
    "            for key, value in losses.items():\n",
    "                output_line += str(value.average) + \" +- \" + str(value.std) + \" & \"\n",
    "                \n",
    "            print(output_line[:-2] + \"\\\\\\\\\")\n",
    "            \n",
    "            if i < len(results)-1:\n",
    "                print(\"\\\\cline{3-4}\")\n",
    "            else:\n",
    "                print(\"\\\\hline\")\n",
    "    \n",
    "    print (\"\\\\end{tabular}\")\n",
    "    print (\"\\\\end{center}\")\n",
    "    print(\"*\"*80)\n",
    "    \n",
    "dataset = \"MNIST\"\n",
    "project_name = \"MTVAEs_05.01-cross-val\"\n",
    "directory = f\"assets/results/raw/{project_name}/{dataset}/\"\n",
    "    \n",
    "output_latex_table_of_results(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
